#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤í¬ë˜í¼: RSS + ì „í†µì  ìŠ¤í¬ë˜í•‘ ê²°í•©
"""

import json
import os
from datetime import datetime
import pytz
from collections import defaultdict

# ì „í†µì  ìŠ¤í¬ë˜í¼ ê°€ì ¸ì˜¤ê¸°
try:
    from scraper import scrape_news_traditional, load_settings, load_sites, get_kst_now, get_kst_now_iso
except ImportError:
    # ìƒëŒ€ ì„í¬íŠ¸ ì‹œë„
    import sys
    sys.path.append(os.path.dirname(__file__))
    from scraper import scrape_news_traditional, load_settings, load_sites, get_kst_now, get_kst_now_iso

def create_basic_summary(title, content):
    """ê¸°ë³¸ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½ ìƒì„±"""
    # ê°„ë‹¨í•œ í•œêµ­ì–´ ìš”ì•½ ìƒì„±
    content_preview = content[:100] + "..." if len(content) > 100 else content
    return f"ğŸ“° ì‹±ê°€í¬ë¥´ ìµœì‹  ë‰´ìŠ¤\nğŸ“¢ {title[:50]}{'...' if len(title) > 50 else ''}"

def scrape_news_hybrid():
    """í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: Traditional ë§í¬ ìˆ˜ì§‘ + AI ìš”ì•½"""
    print("[HYBRID] Starting hybrid scraping (Traditional Links + AI Summary)...")
    
    settings = load_settings()
    sites = load_sites()
    
    # Phase 1: Traditional ë°©ì‹ìœ¼ë¡œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
    print("\n[HYBRID] Phase 1: Traditional Link Collection")
    articles_by_group = defaultdict(list)
    
    # Traditional ìŠ¤í¬ë˜í¼ë¡œ ê¸°ì‚¬ ë§í¬ì™€ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
    try:
        traditional_file = scrape_news_traditional()
        if traditional_file and os.path.exists(traditional_file):
            # Traditional ìŠ¤í¬ë˜í•‘ ê²°ê³¼ íŒŒì¼ ì½ê¸°
            with open(traditional_file, 'r', encoding='utf-8') as f:
                traditional_articles = json.load(f)
            
            # ê¸°ì‚¬ ë°ì´í„°ë¥¼ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ë¡œ ë³€í™˜
            for group_data in traditional_articles:
                group = group_data['group']
                for article in group_data['articles']:
                    articles_by_group[group].append({
                        'site': article['site'],
                        'title': article['title'],
                        'url': article['url'],
                        'content': article['content'],
                        'publish_date': article['publish_date'],
                        'extracted_by': 'traditional'
                    })
            print(f"[HYBRID] Traditional: Loaded articles from {len(traditional_articles)} groups")
        else:
            print("[HYBRID] Traditional scraping returned no file or file not found")
    except Exception as e:
        print(f"[HYBRID] Traditional scraping error: {e}")
        import traceback
        traceback.print_exc()
    
    # Phase 2: AI ìš”ì•½ ìƒì„±
    print("\n[HYBRID] Phase 2: AI Summary Generation")
    
    # AI ìš”ì•½ì´ ê°€ëŠ¥í•œì§€ í™•ì¸ (Cohere ë˜ëŠ” Gemini)
    cohere_available = bool(os.environ.get('COHERE_API_KEY'))
    gemini_available = bool(os.environ.get('GOOGLE_GEMINI_API_KEY'))
    ai_available = cohere_available or gemini_available
    
    print(f"[HYBRID] Cohere API available: {cohere_available}")
    print(f"[HYBRID] Gemini API available: {gemini_available}")
    
    if ai_available:
        try:
            # AI ìš”ì•½ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹œë„
            try:
                from ai_summary_simple import translate_to_korean_summary_cohere, translate_to_korean_summary_gemini
            except ImportError:
                import sys
                sys.path.append(os.path.dirname(__file__))
                from ai_summary_simple import translate_to_korean_summary_cohere, translate_to_korean_summary_gemini
            
            # ê° ê¸°ì‚¬ì— ëŒ€í•´ AI ìš”ì•½ ìƒì„±
            ai_count = 0
            max_ai_summaries = 25  # ìµœëŒ€ AI ìš”ì•½ ìˆ˜ ì œí•œ (Cohere ì›” 1000ê°œ ì œí•œ ê³ ë ¤)
            
            for group, group_articles in articles_by_group.items():
                for article in group_articles:
                    if ai_count >= max_ai_summaries:
                        print(f"[HYBRID] AI summary limit reached ({max_ai_summaries})")
                        article['summary'] = create_basic_summary(article['title'], article['content'])
                        continue
                        
                    try:
                        ai_summary = None
                        api_used = None
                        
                        # Cohere API ìš°ì„  ì‹œë„
                        if cohere_available and ai_count < max_ai_summaries:
                            print(f"[HYBRID] Trying Cohere API for: {article['title'][:50]}...")
                            ai_summary = translate_to_korean_summary_cohere(
                                article['title'], 
                                article['content']
                            )
                            if ai_summary:
                                api_used = 'cohere'
                                ai_count += 1
                        
                        # Cohere ì‹¤íŒ¨ì‹œ Gemini ì‹œë„
                        if not ai_summary and gemini_available and ai_count < max_ai_summaries:
                            print(f"[HYBRID] Trying Gemini API for: {article['title'][:50]}...")
                            ai_summary = translate_to_korean_summary_gemini(
                                article['title'], 
                                article['content']
                            )
                            if ai_summary:
                                api_used = 'gemini'
                                ai_count += 1
                        
                        if ai_summary:
                            article['summary'] = ai_summary
                            article['extracted_by'] = f'hybrid_{api_used}'
                            print(f"[HYBRID] AI summary generated using {api_used} ({ai_count}/{max_ai_summaries})")
                        else:
                            # AI ìš”ì•½ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ìš”ì•½ ì‚¬ìš©
                            article['summary'] = create_basic_summary(article['title'], article['content'])
                            article['extracted_by'] = 'hybrid_fallback'
                            print(f"[HYBRID] Using basic summary for: {article['title'][:50]}...")
                            
                    except Exception as e:
                        print(f"[HYBRID] AI summary error for {article['title'][:30]}: {e}")
                        article['summary'] = create_basic_summary(article['title'], article['content'])
                        article['extracted_by'] = 'hybrid_error'
        except ImportError as e:
            print(f"[HYBRID] AI summary module import error: {e}")
            ai_available = False
    
    if not ai_available:
        print("[HYBRID] Using basic keyword summaries")
        for group, group_articles in articles_by_group.items():
            for article in group_articles:
                article['summary'] = create_basic_summary(article['title'], article['content'])
    
    # Phase 3: ê²°ê³¼ í†µí•© ë° ìµœì¢… ì²˜ë¦¬
    print("\n[HYBRID] Phase 3: Final Processing")
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
            
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in group_articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 5ê°œì˜ ê¸°ì‚¬ (í•˜ì´ë¸Œë¦¬ë“œëŠ” ë” ë§ì´)
        selected_articles = unique_articles[:5]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': get_kst_now_iso(),
            'scraping_method': 'hybrid_ai',
            'execution_type': 'auto'
        }
        
        consolidated_articles.append(group_summary)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = get_kst_now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    # latest.json íŒŒì¼ ì—…ë°ì´íŠ¸
    latest_info = {
        'lastUpdated': get_kst_now_iso(),
        'latestFile': f'news_{timestamp}.json',
        'scrapingMethod': 'hybrid_ai',
        'executionType': 'auto'
    }
    with open('data/latest.json', 'w', encoding='utf-8') as f:
        json.dump(latest_info, f, ensure_ascii=False, indent=2)
    
    # ê²°ê³¼ ìš”ì•½
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    total_sites = len(set(article['site'] for group in consolidated_articles for article in group['articles']))
    
    print(f"\n[HYBRID] === Final Results ===")
    print(f"[HYBRID] Scraping Method: Traditional Links + AI Summary")
    print(f"[HYBRID] Total articles: {total_articles}")
    print(f"[HYBRID] Total sites: {total_sites}")
    print(f"[HYBRID] AI summaries: {sum(1 for group in consolidated_articles for article in group['articles'] if article.get('extracted_by') == 'hybrid_ai')}")
    print(f"[HYBRID] Basic summaries: {sum(1 for group in consolidated_articles for article in group['articles'] if article.get('extracted_by') != 'hybrid_ai')}")
    print(f"[HYBRID] Output: {output_file}")
    
    return output_file

if __name__ == "__main__":
    output_file = scrape_news_hybrid()
    print(f"\n[HYBRID] Scraping completed.")