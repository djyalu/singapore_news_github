#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS í”¼ë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼
"""

import feedparser
import requests
from datetime import datetime, timedelta
import json
import os
from collections import defaultdict
from urllib.parse import urlparse
import re

# RSS í”¼ë“œ ëª©ë¡
RSS_FEEDS = {
    'Mothership': 'https://mothership.sg/feed/',
    'Singapore Business Review': 'https://sbr.com.sg/news.rss',
    'Channel NewsAsia': 'https://www.channelnewsasia.com/api/v1/rss-outbound-feed?_format=xml',
    'The Independent Singapore': 'https://theindependent.sg/feed/',
    # ì¶”ê°€ í”¼ë“œë“¤ì„ ì—¬ê¸°ì— ë„£ì„ ìˆ˜ ìˆìŒ
}

# ê·¸ë£¹ ë§¤í•‘
SITE_GROUP_MAPPING = {
    'Mothership': 'Lifestyle',
    'Singapore Business Review': 'Economy',
    'Channel NewsAsia': 'News',
    'The Independent Singapore': 'Politics',
}

def load_settings():
    """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    try:
        # Vercel API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
        api_url = 'https://singapore-news-github.vercel.app/api/auth.js?type=config'
        response = requests.get(api_url, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('success') and data.get('settings'):
                print("[SETTINGS] Loaded settings from server API")
                return data['settings']
    except Exception as e:
        print(f"[SETTINGS] Failed to load from API: {e}")
    
    # ë¡œì»¬ íŒŒì¼ í´ë°±
    try:
        with open('data/settings.json', 'r', encoding='utf-8') as f:
            print("[SETTINGS] Loaded settings from local file")
            return json.load(f)
    except:
        print("[SETTINGS] Using default settings")
        return {
            'scrapTarget': 'all',
            'blockedKeywords': '',
            'importantKeywords': 'singapore,ì‹±ê°€í¬ë¥´',
            'maxArticlesPerSite': 3
        }

def is_recent_article(pub_date, hours=24):
    """ìµœê·¼ ê¸°ì‚¬ì¸ì§€ í™•ì¸"""
    if not pub_date:
        return True
    
    try:
        # ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(pub_date, str):
            pub_date = datetime.fromisoformat(pub_date.replace('Z', '+00:00'))
        
        now = datetime.now(pub_date.tzinfo) if pub_date.tzinfo else datetime.now()
        time_diff = now - pub_date
        return time_diff.total_seconds() < (hours * 3600)
    except:
        return True

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ''
    
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # ì¤‘ë³µ ê³µë°± ì œê±°
    text = ' '.join(text.split())
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = text.replace('&nbsp;', ' ').replace('&amp;', '&')
    text = text.replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&quot;', '"').replace('&#39;', "'")
    
    return text.strip()

def create_keyword_summary(title, content):
    """í‚¤ì›Œë“œ ê¸°ë°˜ í•œê¸€ ìš”ì•½"""
    keywords = {
        'singapore': 'ì‹±ê°€í¬ë¥´', 'economy': 'ê²½ì œ', 'government': 'ì •ë¶€',
        'education': 'êµìœ¡', 'health': 'ë³´ê±´', 'transport': 'êµí†µ',
        'technology': 'ê¸°ìˆ ', 'business': 'ë¹„ì¦ˆë‹ˆìŠ¤', 'covid': 'ì½”ë¡œë‚˜',
        'minister': 'ì¥ê´€', 'policy': 'ì •ì±…', 'development': 'ê°œë°œ',
        'housing': 'ì£¼íƒ', 'hdb': 'ì£¼íƒ', 'property': 'ë¶€ë™ì‚°',
        'police': 'ë²•ë¥ ', 'court': 'ë²•ë¥ ', 'crime': 'ë²”ì£„',
        'market': 'ê²½ì œ', 'investment': 'ê²½ì œ', 'trade': 'ê²½ì œ',
        'malaysia': 'êµ­ì œ', 'indonesia': 'êµ­ì œ', 'asean': 'êµ­ì œ'
    }
    
    found_keywords = []
    text_lower = (title + ' ' + content).lower()
    
    for eng, kor in keywords.items():
        if eng in text_lower:
            found_keywords.append(kor)
    
    found_keywords = list(dict.fromkeys(found_keywords))[:3]
    
    if found_keywords:
        summary = f"ğŸ“° {', '.join(found_keywords)} ê´€ë ¨ ë‰´ìŠ¤"
    else:
        summary = f"ğŸ“° ì‹±ê°€í¬ë¥´ ìµœì‹  ë‰´ìŠ¤"
    
    summary += f"\nğŸ“¢ {title[:80]}{'...' if len(title) > 80 else ''}"
    
    return summary

def scrape_rss_feed(feed_url, site_name, settings):
    """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
    articles = []
    
    try:
        print(f"\n[RSS] Fetching feed from {site_name}: {feed_url}")
        
        # feedparserë¡œ RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(feed_url)
        
        if feed.bozo:
            print(f"[RSS] Warning: Feed parsing error for {site_name}: {feed.bozo_exception}")
        
        print(f"[RSS] Found {len(feed.entries)} entries in {site_name} feed")
        
        # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
        max_articles = int(settings.get('maxArticlesPerSite', 3))
        blocked_keywords = [kw.strip() for kw in settings.get('blockedKeywords', '').split(',') if kw.strip()]
        important_keywords = [kw.strip() for kw in settings.get('importantKeywords', '').split(',') if kw.strip()]
        
        for entry in feed.entries[:max_articles * 2]:  # í•„í„°ë§ì„ ê³ ë ¤í•´ ë” ë§ì´ ê°€ì ¸ì˜´
            try:
                # ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
                title = clean_text(entry.get('title', ''))
                
                # ë‚´ìš© ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œ ì‹œë„)
                content = ''
                for field in ['summary', 'description', 'content']:
                    if hasattr(entry, field):
                        if field == 'content' and isinstance(entry.content, list):
                            content = clean_text(entry.content[0].get('value', ''))
                        else:
                            content = clean_text(getattr(entry, field, ''))
                        if content:
                            break
                
                if not title or not content:
                    continue
                
                # URL ì¶”ì¶œ
                url = entry.get('link', '')
                if not url:
                    continue
                
                # ë°œí–‰ì¼ ì¶”ì¶œ
                pub_date = None
                for date_field in ['published_parsed', 'updated_parsed']:
                    if hasattr(entry, date_field) and getattr(entry, date_field):
                        try:
                            pub_date = datetime(*getattr(entry, date_field)[:6])
                            break
                        except:
                            pass
                
                if not pub_date:
                    pub_date = datetime.now()
                
                # í•„í„°ë§
                full_text = f"{title} {content}"
                
                # ì°¨ë‹¨ í‚¤ì›Œë“œ í™•ì¸
                if blocked_keywords and any(kw.lower() in full_text.lower() for kw in blocked_keywords):
                    print(f"[RSS] Blocked by keywords: {title[:50]}...")
                    continue
                
                # ìµœê·¼ ê¸°ì‚¬ í™•ì¸
                if settings['scrapTarget'] == 'recent' and not is_recent_article(pub_date):
                    print(f"[RSS] Not recent: {title[:50]}...")
                    continue
                
                # ì¤‘ìš” í‚¤ì›Œë“œ í™•ì¸
                if settings['scrapTarget'] == 'important' and important_keywords:
                    if not any(kw.lower() in full_text.lower() for kw in important_keywords):
                        print(f"[RSS] No important keywords: {title[:50]}...")
                        continue
                
                # ìš”ì•½ ìƒì„±
                summary = create_keyword_summary(title, content)
                
                # ê¸°ì‚¬ ì¶”ê°€
                article = {
                    'site': site_name,
                    'title': title,
                    'url': url,
                    'summary': summary,
                    'content': content[:1000],  # ìµœëŒ€ 1000ì
                    'publish_date': pub_date.isoformat() if pub_date else datetime.now().isoformat()
                }
                
                articles.append(article)
                print(f"[RSS] Added article: {title[:50]}...")
                
                if len(articles) >= max_articles:
                    break
                    
            except Exception as e:
                print(f"[RSS] Error processing entry: {e}")
                continue
                
    except Exception as e:
        print(f"[RSS] Error fetching feed from {site_name}: {e}")
    
    return articles

def scrape_news_rss():
    """RSS í”¼ë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
    settings = load_settings()
    articles_by_group = defaultdict(list)
    
    # ê° RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
    for site_name, feed_url in RSS_FEEDS.items():
        articles = scrape_rss_feed(feed_url, site_name, settings)
        
        if articles:
            group = SITE_GROUP_MAPPING.get(site_name, 'News')
            articles_by_group[group].extend(articles)
            print(f"[RSS] Collected {len(articles)} articles from {site_name}")
    
    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ í†µí•©
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
        
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in group_articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œì˜ ì£¼ìš” ê¸°ì‚¬ë§Œ ì„ íƒ
        selected_articles = unique_articles[:3]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': datetime.now().isoformat(),
            'scraping_method': 'rss',
            'execution_type': 'manual'
        }
        
        consolidated_articles.append(group_summary)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    # latest.json íŒŒì¼ ì—…ë°ì´íŠ¸
    latest_info = {
        'lastUpdated': datetime.now().isoformat(),
        'latestFile': f'news_{timestamp}.json',
        'scrapingMethod': 'rss',
        'executionType': 'manual'
    }
    with open('data/latest.json', 'w', encoding='utf-8') as f:
        json.dump(latest_info, f, ensure_ascii=False, indent=2)
    
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    print(f"\n[RSS] Total scraped: {total_articles} articles from {len(consolidated_articles)} groups")
    return output_file

if __name__ == "__main__":
    output_file = scrape_news_rss()
    print(f"\n[RSS] Scraping completed. Output: {output_file}")