import json
import os
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
from collections import defaultdict

def load_settings():
    with open('data/settings.json', 'r') as f:
        return json.load(f)

def load_sites():
    with open('data/sites.json', 'r') as f:
        return json.load(f)

def is_recent_article(article_date):
    if not article_date:
        return True
    return (datetime.now() - article_date).days <= 2

def contains_keywords(text, keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in keywords)

def is_blocked(text, blocked_keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in blocked_keywords)

def translate_to_korean_summary(title, content):
    """ê¸°ì‚¬ë¥¼ í•œê¸€ë¡œ ìš”ì•½"""
    # ê°„ë‹¨í•œ ìš”ì•½ ìƒì„± (ì‹¤ì œë¡œëŠ” ë²ˆì—­ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ)
    # ì—¬ê¸°ì„œëŠ” í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ í•œê¸€ í…œí”Œë¦¿ìœ¼ë¡œ í‘œì‹œ
    
    # ì œëª©ì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ
    title_lower = title.lower()
    
    # ìˆ«ì ì¶”ì¶œ (ì˜ˆ: ì„±ì¥ë¥ , ê¸ˆì•¡ ë“±)
    numbers = re.findall(r'\d+\.?\d*%?', title + ' ' + content)
    
    # ì£¼ìš” í‚¤ì›Œë“œ ë§¤í•‘
    keyword_mapping = {
        'economy': 'ê²½ì œ',
        'gdp': 'GDP',
        'growth': 'ì„±ì¥',
        'singapore': 'ì‹±ê°€í¬ë¥´',
        'government': 'ì •ë¶€',
        'business': 'ë¹„ì¦ˆë‹ˆìŠ¤',
        'technology': 'ê¸°ìˆ ',
        'covid': 'ì½”ë¡œë‚˜',
        'pandemic': 'íŒ¬ë°ë¯¹',
        'market': 'ì‹œì¥',
        'investment': 'íˆ¬ì',
        'property': 'ë¶€ë™ì‚°',
        'transport': 'êµí†µ',
        'mrt': 'MRT',
        'education': 'êµìœ¡',
        'health': 'ê±´ê°•',
        'healthcare': 'ì˜ë£Œ',
        'finance': 'ê¸ˆìœµ',
        'bank': 'ì€í–‰',
        'trade': 'ë¬´ì—­',
        'export': 'ìˆ˜ì¶œ',
        'import': 'ìˆ˜ì…'
    }
    
    # í•œê¸€ ìš”ì•½ ìƒì„±
    summary = f"ğŸ“° {title}\n"
    
    # ì£¼ìš” ë‚´ìš© ìš”ì•½ (ì²˜ìŒ 200ì)
    content_preview = content[:200].strip()
    if len(content) > 200:
        content_preview += "..."
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ìš”ì•½
    found_keywords = []
    for eng, kor in keyword_mapping.items():
        if eng in title_lower or eng in content.lower():
            found_keywords.append(kor)
    
    if found_keywords:
        summary += f"ğŸ” ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(found_keywords[:3])}\n"
    
    if numbers:
        summary += f"ğŸ“Š ì£¼ìš” ìˆ˜ì¹˜: {', '.join(numbers[:3])}\n"
    
    summary += f"ğŸ“ {content_preview}"
    
    return summary

def create_summary(article_data, settings):
    """ì„¤ì •ì— ë”°ë¥¸ ìš”ì•½ ìƒì„±"""
    summary_parts = []
    
    # í•œê¸€ ìš”ì•½ ìƒì„±
    korean_summary = translate_to_korean_summary(
        article_data['title'], 
        article_data['content']
    )
    
    return korean_summary

def extract_article_content(url):
    """ê°„ë‹¨í•œ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = ""
        title_selectors = ['h1', '.headline', '.title', 'title']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem and title_elem.get_text().strip():
                title = title_elem.get_text().strip()
                break
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_selectors = [
            'article', '.article-content', '.content', '.story', 
            '.post-content', 'main', '.main-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in content_elem(["script", "style"]):
                    script.decompose()
                content = content_elem.get_text().strip()
                break
        
        # fallback: p íƒœê·¸ë“¤ ìˆ˜ì§‘
        if not content:
            paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in paragraphs[:5]])
        
        return {
            'title': title,
            'content': content[:500],  # ì²˜ìŒ 500ìë§Œ
            'publish_date': datetime.now()
        }
        
    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return None

def scrape_news():
    settings = load_settings()
    sites = load_sites()
    articles_by_group = defaultdict(list)
    
    blocked_keywords = [kw.strip() for kw in settings.get('blockedKeywords', '').split(',') if kw.strip()]
    important_keywords = [kw.strip() for kw in settings.get('importantKeywords', '').split(',') if kw.strip()]
    
    for site in sites:
        try:
            response = requests.get(site['url'], timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
            links = []
            for a in soup.find_all('a', href=True):
                href = a['href']
                if not href.startswith('http'):
                    if href.startswith('/'):
                        href = site['url'].rstrip('/') + href
                    else:
                        href = site['url'].rstrip('/') + '/' + href
                
                # ê¸°ì‚¬ ë§í¬ë¡œ ë³´ì´ëŠ” íŒ¨í„´ í•„í„°ë§
                if any(pattern in href.lower() for pattern in ['article', 'news', 'story', '/20']):
                    links.append(href)
            
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œë¡œ ì œí•œ (ê·¸ë£¹ë³„ í†µí•©ì„ ìœ„í•´ ì¤„ì„)
            links = list(set(links))[:5]
            
            for article_url in links:
                try:
                    article_data = extract_article_content(article_url)
                    if not article_data or not article_data['title']:
                        continue
                    
                    full_text = f"{article_data['title']} {article_data['content']}"
                    
                    if is_blocked(full_text, blocked_keywords):
                        continue
                    
                    if settings['scrapTarget'] == 'recent' and not is_recent_article(article_data['publish_date']):
                        continue
                    
                    if settings['scrapTarget'] == 'important' and not contains_keywords(full_text, important_keywords):
                        continue
                    
                    # ìš”ì•½ ìƒì„±
                    summary = create_summary(article_data, settings)
                    
                    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
                    articles_by_group[site['group']].append({
                        'site': site['name'],
                        'title': article_data['title'],
                        'url': article_url,
                        'summary': summary,
                        'content': article_data['content'],
                        'publish_date': article_data['publish_date'].isoformat() if article_data['publish_date'] else None
                    })
                    
                except Exception as e:
                    print(f"Error processing article {article_url}: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping {site['name']}: {e}")
            continue
    
    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ í†µí•©
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
            
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œì˜ ì£¼ìš” ê¸°ì‚¬ë§Œ ì„ íƒ
        selected_articles = group_articles[:3]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': datetime.now().isoformat()
        }
        
        consolidated_articles.append(group_summary)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    print(f"Scraped {total_articles} articles from {len(consolidated_articles)} groups")
    return output_file

if __name__ == "__main__":
    # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
    scrape_news()
    
    # 30ì¼ ì´ì „ ë°ì´í„° ìë™ ì •ë¦¬
    try:
        from cleanup_old_data import cleanup_old_data
        cleanup_old_data(30)
        print("Old data cleanup completed")
    except ImportError:
        print("cleanup_old_data module not found, skipping cleanup")
    except Exception as e:
        print(f"Error during cleanup: {e}")