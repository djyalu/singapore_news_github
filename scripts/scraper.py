import json
import os
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
from collections import defaultdict
from urllib.parse import urljoin, urlparse

def load_settings():
    with open('data/settings.json', 'r') as f:
        return json.load(f)

def load_sites():
    with open('data/sites.json', 'r') as f:
        return json.load(f)

def is_recent_article(article_date):
    if not article_date:
        return True
    return (datetime.now() - article_date).days <= 2

def contains_keywords(text, keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in keywords)

def is_blocked(text, blocked_keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in blocked_keywords)

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ - HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°"""
    # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    text = text.replace('\r\n', ' ').replace('\n', ' ').replace('\r', ' ')
    # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()

def extract_article_content_straits_times(url, soup):
    """The Straits Times ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1.headline, h1[data-testid="headline"], .article-headline h1')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ë³¸ë¬¸ ì¶”ì¶œ
    content_elem = soup.select_one('div[data-testid="article-body"], .article-content, .paywall-content')
    if content_elem:
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for elem in content_elem.select('.related-articles, .advertisement, script, style'):
            elem.decompose()
        
        paragraphs = content_elem.find_all('p')
        content = ' '.join([clean_text(p.get_text()) for p in paragraphs if p.get_text().strip()])
        article['content'] = content[:1000]  # 1000ìë¡œ ì œí•œ
    
    # ë‚ ì§œ ì¶”ì¶œ
    date_elem = soup.select_one('time, .published-date, [data-testid="publish-date"]')
    if date_elem:
        try:
            if date_elem.get('datetime'):
                article['publish_date'] = datetime.fromisoformat(date_elem['datetime'].replace('Z', '+00:00'))
            else:
                article['publish_date'] = datetime.now()
        except:
            article['publish_date'] = datetime.now()
    
    return article

def extract_article_content_moe(url, soup):
    """Ministry of Education ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1, .page-title, .content-title')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ë³¸ë¬¸ ì¶”ì¶œ - MOEëŠ” ì£¼ë¡œ div.content-area ì‚¬ìš©
    content_elem = soup.select_one('.content-area, .page-content, main')
    if content_elem:
        # ë„¤ë¹„ê²Œì´ì…˜, í—¤ë”, í‘¸í„° ì œê±°
        for elem in content_elem.select('nav, header, footer, .breadcrumb, .sidebar'):
            elem.decompose()
        
        # ì‹¤ì œ ì½˜í…ì¸  ì¶”ì¶œ
        paragraphs = content_elem.find_all(['p', 'li'])
        content_parts = []
        for p in paragraphs:
            text = clean_text(p.get_text())
            if len(text) > 20:  # ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§
                content_parts.append(text)
        
        article['content'] = ' '.join(content_parts[:10])[:1000]
    
    # ë‚ ì§œ ì¶”ì¶œ
    date_text = soup.get_text()
    date_match = re.search(r'(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})', date_text)
    if date_match:
        try:
            date_str = f"{date_match.group(1)} {date_match.group(2)} {date_match.group(3)}"
            article['publish_date'] = datetime.strptime(date_str, "%d %b %Y")
        except:
            pass
    
    return article

def extract_article_content_nac(url, soup):
    """National Arts Council ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # NACëŠ” ì£¼ë¡œ ì´ë²¤íŠ¸ ì •ë³´
    title_elem = soup.select_one('h1, .event-title, .programme-title')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ì´ë²¤íŠ¸ ì„¤ëª… ì¶”ì¶œ
    content_elem = soup.select_one('.event-description, .programme-description, .content-main')
    if content_elem:
        article['content'] = clean_text(content_elem.get_text())[:1000]
    
    # ë‚ ì§œëŠ” ì´ë²¤íŠ¸ ë‚ ì§œë¡œ
    date_elem = soup.select_one('.event-date, .programme-date')
    if date_elem:
        article['publish_date'] = datetime.now()  # ê°„ë‹¨íˆ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
    
    return article

def extract_article_content_generic(url, soup):
    """ë²”ìš© ì½˜í…ì¸  ì¶”ì¶œ (í´ë°±)"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.find('h1')
    if not title_elem:
        title_elem = soup.find('title')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ë³¸ë¬¸ ì¶”ì¶œ
    # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
    main_content = ""
    for elem in soup.find_all(['article', 'main', 'div']):
        text = clean_text(elem.get_text())
        if len(text) > len(main_content):
            main_content = text
    
    article['content'] = main_content[:1000]
    
    return article

def extract_article_content(url):
    """URLì— ë”°ë¼ ì ì ˆí•œ ì¶”ì¶œ ë°©ë²• ì„ íƒ"""
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë„ë©”ì¸ì— ë”°ë¼ ë‹¤ë¥¸ ì¶”ì¶œ ë°©ë²• ì‚¬ìš©
        domain = urlparse(url).netloc.lower()
        
        if 'straitstimes.com' in domain:
            return extract_article_content_straits_times(url, soup)
        elif 'moe.gov.sg' in domain:
            return extract_article_content_moe(url, soup)
        elif 'nac.gov.sg' in domain or 'catch.sg' in domain:
            return extract_article_content_nac(url, soup)
        else:
            return extract_article_content_generic(url, soup)
            
    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return None

def get_article_links_straits_times(soup, base_url):
    """The Straits Times ì „ìš© ë§í¬ ì¶”ì¶œ"""
    links = []
    
    # ì‹¤ì œ ê¸°ì‚¬ ë§í¬ íŒ¨í„´
    article_patterns = [
        r'/singapore/',
        r'/asia/',
        r'/world/',
        r'/business/',
        r'/sport/',
        r'/lifestyle/',
        r'/multimedia/'
    ]
    
    for a in soup.select('a[href]'):
        href = a.get('href', '')
        full_url = urljoin(base_url, href)
        
        # ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
        if any(pattern in href for pattern in article_patterns):
            # ì œì™¸í•  íŒ¨í„´
            exclude_patterns = ['subscribe', 'login', 'register', 'terms', 'privacy']
            if not any(exclude in href.lower() for exclude in exclude_patterns):
                links.append(full_url)
    
    return list(set(links))[:10]  # ì¤‘ë³µ ì œê±° í›„ 10ê°œê¹Œì§€

def get_article_links_moe(soup, base_url):
    """MOE ì „ìš© ë§í¬ ì¶”ì¶œ"""
    links = []
    
    # MOEëŠ” ì£¼ë¡œ press-releasesì™€ news ì„¹ì…˜
    for a in soup.select('a[href*="press-releases"], a[href*="/news/"]'):
        href = a.get('href', '')
        full_url = urljoin(base_url, href)
        
        # ì‹¤ì œ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (ë‚ ì§œ íŒ¨í„´ í¬í•¨)
        if re.search(r'/20\d{2}/', href):
            links.append(full_url)
    
    return list(set(links))[:10]

def get_article_links_generic(soup, base_url):
    """ë²”ìš© ë§í¬ ì¶”ì¶œ"""
    links = []
    
    for a in soup.select('a[href]'):
        href = a.get('href', '')
        full_url = urljoin(base_url, href)
        
        # ê¸°ë³¸ í•„í„°ë§
        if any(pattern in href.lower() for pattern in ['article', 'news', 'story', '/20']):
            links.append(full_url)
    
    return list(set(links))[:10]

def create_summary(article_data, settings):
    """ì„¤ì •ì— ë”°ë¥¸ ìš”ì•½ ìƒì„±"""
    try:
        # ë¬´ë£Œ AI ìš”ì•½ ì‹œë„
        from ai_summary_free import get_free_summary
        summary = get_free_summary(
            article_data['title'], 
            article_data['content']
        )
        # Gemini API ì‚¬ìš© ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if summary and not summary.startswith('ğŸ“° ' + article_data['title']):
            return summary
    except Exception as e:
        print(f"AI summary error: {e}")
    
    # í´ë°±: í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½
    return create_keyword_summary(article_data['title'], article_data['content'])

def create_keyword_summary(title, content):
    """í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ í•œê¸€ ìš”ì•½"""
    # í‚¤ì›Œë“œ ë§¤í•‘
    keywords = {
        'singapore': 'ì‹±ê°€í¬ë¥´', 'economy': 'ê²½ì œ', 'government': 'ì •ë¶€',
        'education': 'êµìœ¡', 'health': 'ë³´ê±´', 'transport': 'êµí†µ',
        'technology': 'ê¸°ìˆ ', 'business': 'ë¹„ì¦ˆë‹ˆìŠ¤', 'covid': 'ì½”ë¡œë‚˜',
        'minister': 'ì¥ê´€', 'policy': 'ì •ì±…', 'development': 'ê°œë°œ'
    }
    
    # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    found_keywords = []
    text_lower = (title + ' ' + content).lower()
    
    for eng, kor in keywords.items():
        if eng in text_lower:
            found_keywords.append(kor)
    
    # ìš”ì•½ ìƒì„±
    if found_keywords:
        summary = f"ğŸ“° {', '.join(found_keywords[:3])} ê´€ë ¨ ë‰´ìŠ¤\n"
    else:
        summary = f"ğŸ“° ì‹±ê°€í¬ë¥´ ìµœì‹  ë‰´ìŠ¤\n"
    
    # ë‚´ìš© ìš”ì•½
    sentences = content.split('.')[:2]  # ì²˜ìŒ 2ë¬¸ì¥
    if sentences:
        summary += f"ğŸ“ {'. '.join(sentences).strip()}..."
    
    return summary

def scrape_news():
    settings = load_settings()
    sites = load_sites()
    articles_by_group = defaultdict(list)
    
    blocked_keywords = [kw.strip() for kw in settings.get('blockedKeywords', '').split(',') if kw.strip()]
    important_keywords = [kw.strip() for kw in settings.get('importantKeywords', '').split(',') if kw.strip()]
    
    for site in sites:
        try:
            print(f"Scraping {site['name']}...")
            response = requests.get(site['url'], timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì‚¬ì´íŠ¸ë³„ ë§í¬ ì¶”ì¶œ
            domain = urlparse(site['url']).netloc.lower()
            if 'straitstimes.com' in domain:
                links = get_article_links_straits_times(soup, site['url'])
            elif 'moe.gov.sg' in domain:
                links = get_article_links_moe(soup, site['url'])
            else:
                links = get_article_links_generic(soup, site['url'])
            
            print(f"Found {len(links)} article links")
            
            # ê¸°ì‚¬ë³„ ì²˜ë¦¬
            for article_url in links[:5]:  # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 5ê°œ
                try:
                    print(f"  Processing: {article_url}")
                    article_data = extract_article_content(article_url)
                    
                    if not article_data or not article_data['title'] or len(article_data['content']) < 50:
                        continue
                    
                    full_text = f"{article_data['title']} {article_data['content']}"
                    
                    # í•„í„°ë§
                    if is_blocked(full_text, blocked_keywords):
                        continue
                    
                    if settings['scrapTarget'] == 'recent' and not is_recent_article(article_data['publish_date']):
                        continue
                    
                    if settings['scrapTarget'] == 'important' and not contains_keywords(full_text, important_keywords):
                        continue
                    
                    # ìš”ì•½ ìƒì„±
                    summary = create_summary(article_data, settings)
                    
                    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
                    articles_by_group[site['group']].append({
                        'site': site['name'],
                        'title': article_data['title'],
                        'url': article_url,
                        'summary': summary,
                        'content': article_data['content'],
                        'publish_date': article_data['publish_date'].isoformat() if article_data['publish_date'] else None
                    })
                    
                except Exception as e:
                    print(f"Error processing article {article_url}: {e}")
                    continue
                    
        except Exception as e:
            print(f"Error scraping {site['name']}: {e}")
            continue
    
    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ í†µí•©
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
            
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in group_articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œì˜ ì£¼ìš” ê¸°ì‚¬ë§Œ ì„ íƒ
        selected_articles = unique_articles[:3]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': datetime.now().isoformat()
        }
        
        consolidated_articles.append(group_summary)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    # latest.json íŒŒì¼ ì—…ë°ì´íŠ¸
    latest_info = {
        'lastUpdated': datetime.now().isoformat(),
        'latestFile': f'news_{timestamp}.json'
    }
    with open('data/latest.json', 'w', encoding='utf-8') as f:
        json.dump(latest_info, f, ensure_ascii=False, indent=2)
    
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    print(f"\nScraped {total_articles} articles from {len(consolidated_articles)} groups")
    return output_file

if __name__ == "__main__":
    scrape_news()