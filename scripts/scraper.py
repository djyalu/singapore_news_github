import json
import os
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
import re
from collections import defaultdict
from urllib.parse import urljoin, urlparse
from ai_scraper import ai_scraper
from text_processing import TextProcessor
from deduplication import ArticleDeduplicator

# ë””ë²„ê·¸ ëª¨ë“œ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ì œì–´)
DEBUG_MODE = os.environ.get('DEBUG_SCRAPER', 'false').lower() == 'true'

def load_settings():
    """ì„œë²„ì—ì„œ ë™ì ìœ¼ë¡œ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ê¸°"""
    try:
        # ë¨¼ì € ì„œë²„ APIì—ì„œ ìµœì‹  ì„¤ì • ì‹œë„
        api_url = "https://singapore-news-github.vercel.app/api/save-data?type=settings"
        response = requests.get(api_url, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success') and result.get('data'):
                print("[SETTINGS] Loaded settings from server API")
                return result['data']
        
        print("[SETTINGS] Server API failed, using local file")
    except Exception as e:
        print(f"[SETTINGS] Server API error: {e}, using local file")
    
    # API ì‹¤íŒ¨ì‹œ ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
    try:
        with open('data/settings.json', 'r') as f:
            print("[SETTINGS] Loaded settings from local file")
            return json.load(f)
    except Exception as e:
        print(f"[SETTINGS] Local file error: {e}, using default settings")
        # ê¸°ë³¸ ì„¤ì • ë°˜í™˜
        return {
            "scrapTarget": "recent",
            "importantKeywords": "",
            "summaryOptions": {"headline": True, "keywords": True, "content": True},
            "sendChannel": "whatsapp",
            "whatsappChannel": "",
            "sendSchedule": {"period": "daily", "time": "08:00", "weekdays": [], "date": "1"},
            "blockedKeywords": "",
            "scrapingMethod": "traditional",
            "scrapingMethodOptions": {
                "ai": {"provider": "gemini", "model": "gemini-1.5-flash", "fallbackToTraditional": True},
                "traditional": {"useEnhancedFiltering": True}
            },
            "monitoring": {"enabled": True}
        }

def load_sites():
    """ì„œë²„ì—ì„œ ë™ì ìœ¼ë¡œ ì‚¬ì´íŠ¸ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜¤ê¸°"""
    try:
        # ë¨¼ì € ì„œë²„ APIì—ì„œ ìµœì‹  ì‚¬ì´íŠ¸ ì„¤ì • ì‹œë„
        api_url = "https://singapore-news-github.vercel.app/api/save-data?type=sites"
        response = requests.get(api_url, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success') and result.get('data'):
                print("[SITES] Loaded sites from server API")
                return result['data']
        
        print("[SITES] Server API failed, using local file")
    except Exception as e:
        print(f"[SITES] Server API error: {e}, using local file")
    
    # API ì‹¤íŒ¨ì‹œ ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
    try:
        with open('data/sites.json', 'r') as f:
            print("[SITES] Loaded sites from local file")
            return json.load(f)
    except Exception as e:
        print(f"[SITES] Local file error: {e}, using empty sites list")
        return []

def is_recent_article(article_date):
    if not article_date:
        return True
    return (datetime.now() - article_date).days <= 2

def contains_keywords(text, keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in keywords)

def is_blocked(text, blocked_keywords):
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in blocked_keywords)

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ - HTML íƒœê·¸, íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°"""
    # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½
    text = text.replace('\r\n', ' ').replace('\n', ' ').replace('\r', ' ')
    # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'\s+', ' ', text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return text.strip()

def is_meaningful_content(text):
    """ì˜ë¯¸ìˆëŠ” ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ í™•ì¸ - ë” ê´€ëŒ€í•˜ê²Œ"""
    if len(text) < 20:
        return False
    
    # ë©”ë‰´ í…ìŠ¤íŠ¸ ì²´í¬ - ë” ì—„ê²©í•˜ê²Œ (í™•ì‹¤í•œ ë©”ë‰´ë§Œ)
    if is_menu_text(text) and len(text) < 100:
        return False
    
    # ê¸°ë³¸ êµ¬ì¡° ì²´í¬
    sentences = text.split('.')
    if len(sentences) < 2:
        return False
    
    # ë‹¨ì–´ ìˆ˜ ì²´í¬
    words = text.split()
    if len(words) < 30:
        return False
    
    # ì§„ì—„í•œ ê¸°ì‚¬ ë‚´ìš© íŒ¨í„´ - ë” ë„“ì€ ë²”ìœ„
    article_indicators = [
        'said', 'announced', 'reported', 'according to', 'in a statement',
        'the government', 'the ministry', 'officials', 'spokesperson',
        'singapore', 'minister', 'prime minister', 'parliament',
        'economic', 'policy', 'development', 'growth', 'investment',
        'will', 'would', 'can', 'could', 'should', 'may', 'might',
        'new', 'project', 'plan', 'programme', 'service', 'system',
        'million', 'billion', 'percent', 'year', 'month', 'week',
        'company', 'business', 'market', 'price', 'cost', 'budget',
        'people', 'public', 'residents', 'citizens', 'community'
    ]
    
    text_lower = text.lower()
    article_score = sum(1 for indicator in article_indicators if indicator in text_lower)
    
    # ë¬¸ì¥ êµ¬ì¡° ì²´í¬ (ë§ˆì¹¨í‘œ, ì‰¼í‘œ ë“±)
    punctuation_score = text.count('.') + text.count(',') + text.count(';')
    
    # ì „ì²´ ê¸¸ì´ ëŒ€ë¹„ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ ë¹„ìœ¨
    meaningful_words = [w for w in words if len(w) > 3 and w.isalpha()]
    
    if len(words) == 0:
        return False
    
    meaningful_ratio = len(meaningful_words) / len(words)
    
    # ì ìˆ˜ ê¸°ë°˜ íŒë‹¨ - ë” ê´€ëŒ€í•˜ê²Œ
    total_score = article_score + (punctuation_score * 0.3) + (meaningful_ratio * 5)
    
    return total_score > 2  # ì ìˆ˜ ê¸°ì¤€ì„ 3ì—ì„œ 2ë¡œ ë‚®ì¶¤

def post_process_article_content(article_data):
    """ì¶”ì¶œëœ ê¸°ì‚¬ ë‚´ìš© í›„ì²˜ë¦¬ - ê°œì„ ëœ ì¤‘ë³µ ì œê±°"""
    if not article_data or not article_data.get('content'):
        return article_data
    
    content = article_data['content']
    
    # TextProcessorë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹¨ë—í•œ ë¬¸ì¥ ì¶”ì¶œ
    sentences = TextProcessor.extract_sentences(content, min_length=20)
    
    # ì¤‘ë³µ ì œê±° (ì •ê·œí™”ëœ ë¹„êµ)
    unique_sentences = []
    seen_normalized = set()
    
    for sentence in sentences:
        # ë©”ë‰´ ë¬¸ì¥ í•„í„°ë§
        if is_menu_sentence(sentence):
            continue
            
        # ì¶”ê°€ ì •ì œ
        cleaned_sentence = clean_menu_remnants(sentence)
        if not cleaned_sentence or len(cleaned_sentence) < 20:
            continue
            
        # ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì²´í¬
        normalized = cleaned_sentence.lower().strip()
        if normalized not in seen_normalized:
            seen_normalized.add(normalized)
            unique_sentences.append(cleaned_sentence)
    
    # ì•ˆì „í•˜ê²Œ ë³‘í•© ë° ìë¥´ê¸°
    if unique_sentences:
        article_data['content'] = TextProcessor.merge_paragraphs(unique_sentences, max_length=1000)
    else:
        article_data['content'] = ''
    
    return article_data

def is_menu_sentence(sentence):
    """ë©¤ë‰´ ë¬¸ì¥ì¸ì§€ ë” ì—„ê²©í•˜ê²Œ íŒë‹¨"""
    sentence_lower = sentence.lower().strip()
    
    # ì§ì ‘ì ì¸ ë©”ë‰´ íŒ¨í„´
    direct_menu_patterns = [
        'ë‚´ í”¼ë“œ ì—ë””ì…˜ ë©”ë‰´',
        'sign in account my feed',
        'edition menu edition',
        'ì‹±ê°€í¬ë¥´ ì¸ë„ë„¤ì‹œì•„ ì•„ì‹œì•„',
        'singapore indonesia asia',
        'cna ë¼ì´í”„ ìŠ¤íƒ€ì¼ ëŸ­ì…”ë¦¬',
        'cna lifestyle luxury',
        'top stories', 'latest news', 'live tv',
        'news id', 'type landing_page',
        'search menu search edition'
    ]
    
    # ì§ì ‘ ë§¤ì¹­
    if any(pattern in sentence_lower for pattern in direct_menu_patterns):
        return True
    
    # íŒ¨í„´ ê¸°ë°˜ íŒë‹¨
    words = sentence_lower.split()
    
    # ì§„ì—„í•œ ë©”ë‰´ ë‹¨ì–´ ë¹„ìœ¨
    menu_words = [
        'feed', 'edition', 'menu', 'account', 'sign', 'search',
        'lifestyle', 'luxury', 'today', 'stories', 'news',
        'singapore', 'indonesia', 'asia', 'cna', 'cnar'
    ]
    
    if len(words) > 0:
        menu_word_ratio = sum(1 for word in words if word in menu_words) / len(words)
        if menu_word_ratio > 0.6:  # 60% ì´ìƒì´ ë©”ë‰´ ë‹¨ì–´
            return True
    
    # ì§„ì—ˆë‹¨ ë‚˜ì—´ íŒ¨í„´ (a b c d e f...)
    if len(words) > 8 and len([w for w in words if len(w) < 4]) > 6:
        return True
    
    return False

def clean_menu_remnants(text):
    """ë‚¨ì€ ë©”ë‰´ ì”ì—¬ë¬¼ ì œê±°"""
    # íŠ¹ì • ë©”ë‰´ ë¬¸êµ¬ ì œê±°
    menu_phrases_to_remove = [
        'ë‚´ í”¼ë“œ ì—ë””ì…˜ ë©”ë‰´ ì—ë””ì…˜ ê³„ì •',
        'Sign In Account My Feed Edition Menu',
        'Edition: Singapore Indonesia Asia',
        'CNAR ê²€ìƒ‰ ë©”ë‰´ ê²€ìƒ‰',
        'CNA Lifestyle Luxury TODAY',
        'News Id', 'Type landing_page'
    ]
    
    cleaned_text = text
    for phrase in menu_phrases_to_remove:
        cleaned_text = cleaned_text.replace(phrase, '')
        cleaned_text = cleaned_text.replace(phrase.lower(), '')
        cleaned_text = cleaned_text.replace(phrase.upper(), '')
    
    # ë‹¤ì¤‘ ê³µë°± ì œê±°
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    return cleaned_text

def is_landing_page_content(content):
    """ëœë”© í˜ì´ì§€ ë˜ëŠ” ë©”ë‰´ í˜ì´ì§€ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨ - ë” ê´€ëŒ€í•˜ê²Œ"""
    if not content or len(content) < 20:
        return True
    
    content_lower = content.lower()
    
    # í™•ì‹¤í•œ ëœë”© í˜ì´ì§€ ì§€í‘œë“¤ë§Œ ì²´í¬
    definite_landing_indicators = [
        'sign in account my feed edition menu',
        'type landing_page',
        'news id 1822271 type landing_page',
        'top stories id 1821936 type landing_page',
        'latest news id 1822271 type landing_page'
    ]
    
    # í™•ì‹¤í•œ ì§€í‘œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ëœë”© í˜ì´ì§€
    if any(indicator in content_lower for indicator in definite_landing_indicators):
        return True
    
    # ë©”ë‰´ í…ìŠ¤íŠ¸ íŒ¨í„´ í™•ì¸ - ë” ì—„ê²©í•˜ê²Œ
    menu_patterns = [
        r'id \d+ type landing_page',
        r'sign in account my feed edition menu',
        r'edition menu edition singapore indonesia asia'
    ]
    
    pattern_matches = sum(1 for pattern in menu_patterns if re.search(pattern, content_lower))
    
    # íŒ¨í„´ ë§¤ì¹­ì´ 2ê°œ ì´ìƒì´ë©´ ëœë”© í˜ì´ì§€
    if pattern_matches >= 2:
        return True
    
    # ì „ì²´ ë‹¨ì–´ ìˆ˜ ëŒ€ë¹„ ë©”ë‰´ ë‹¨ì–´ ë¹„ìœ¨ ê³„ì‚° - ë” ê´€ëŒ€í•˜ê²Œ
    words = content_lower.split()
    menu_words = [
        'sign', 'account', 'feed', 'edition', 'menu', 'search',
        'landing_page', 'landing', 'type'
    ]
    
    if len(words) > 0:
        menu_word_ratio = sum(1 for word in words if word in menu_words) / len(words)
        
        # ë©”ë‰´ ë‹¨ì–´ ë¹„ìœ¨ì´ 70% ì´ìƒì´ë©´ ëœë”© í˜ì´ì§€ (ë” ì—„ê²©í•˜ê²Œ)
        if menu_word_ratio > 0.7:
            return True
    
    # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì˜ ì§€í‘œ í™•ì¸
    article_indicators = [
        'said', 'announced', 'reported', 'according to', 'minister', 'government',
        'policy', 'economic', 'business', 'investment', 'development', 'growth',
        'court', 'sentenced', 'charged', 'arrested', 'police', 'trial',
        'company', 'market', 'shares', 'profit', 'revenue', 'customers',
        'singapore', 'malaysian', 'indonesian', 'thai', 'vietnam'
    ]
    
    article_score = sum(1 for indicator in article_indicators if indicator in content_lower)
    
    # ê¸°ì‚¬ ì§€í‘œê°€ ìˆìœ¼ë©´ ì¼ë‹¨ ê¸°ì‚¬ë¡œ íŒë‹¨
    if article_score > 0:
        return False
    
    # ê¸°ì‚¬ ì§€í‘œê°€ ì „í˜€ ì—†ê³  ë©”ë‰´ ë‹¨ì–´ ë¹„ìœ¨ì´ 50% ì´ìƒì´ë©´ ëœë”© í˜ì´ì§€
    if len(words) > 0:
        menu_word_ratio = sum(1 for word in words if word in menu_words) / len(words)
        if menu_word_ratio > 0.5:
            return True
    
    return False

def validate_final_article_content(article_data):
    """ìµœì¢… ê¸°ì‚¬ ë‚´ìš© ìœ íš¨ì„± ê²€ì‚¬ - ë” ê´€ëŒ€í•˜ê²Œ"""
    if not article_data or not article_data.get('title') or not article_data.get('content'):
        return False
    
    title = article_data['title'].strip()
    content = article_data['content'].strip()
    
    print(f"[DEBUG] Final validation for: {title}")
    
    # ì œëª© ê²€ì‚¬ - ëª…ë°±í•œ ë©”ë‰´/ë„¤ë¹„ê²Œì´ì…˜ ì œëª©ë“¤ë§Œ
    invalid_titles = [
        'newsletters', 'breaking news', 'sign up', 'login', 'register',
        'share on whatsapp', 'yoursingapore story', 'featured',
        'menu', 'search', 'edition'
    ]
    
    if any(invalid in title.lower() for invalid in invalid_titles):
        print(f"[DEBUG] Invalid title detected: {title}")
        return False
    
    # ë‚´ìš© ê²€ì‚¬ - ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ íŠ¹ì§• í™•ì¸ (ë” ê´€ëŒ€í•˜ê²Œ)
    news_indicators = [
        'said', 'announced', 'reported', 'according to', 'minister',
        'government', 'policy', 'singapore', 'parliament', 'court',
        'arrested', 'charged', 'sentenced', 'company', 'business',
        'economy', 'investment', 'market', 'development', 'residents',
        'citizens', 'public', 'authorities', 'officials', 'plan',
        'project', 'programme', 'service', 'system', 'will', 'would',
        'can', 'could', 'should', 'million', 'billion', 'percent',
        'year', 'years', 'month', 'months', 'week', 'weeks', 'day', 'days'
    ]
    
    content_lower = content.lower()
    news_score = sum(1 for indicator in news_indicators if indicator in content_lower)
    
    # ë‰´ìŠ¤ ì§€í‘œê°€ ì „í˜€ ì—†ìœ¼ë©´ ê¸°ì‚¬ê°€ ì•„ë‹˜ (í•˜ì§€ë§Œ ë” ê´€ëŒ€í•˜ê²Œ)
    if news_score == 0:
        # ê¸°ë³¸ì ì¸ ë¬¸ì„œ êµ¬ì¡°ê°€ ìˆëŠ”ì§€ í™•ì¸
        sentences = content.split('.')
        if len(sentences) < 2 or len(content.split()) < 20:
            print(f"[DEBUG] No news indicators and insufficient structure")
            return False
    
    # ë©”ë‰´ í…ìŠ¤íŠ¸ í™•ì¸ - ë” ì—„ê²©í•˜ê²Œ (í™•ì‹¤í•œ ë©”ë‰´ë§Œ)
    if is_menu_text(content) and news_score == 0:
        print(f"[DEBUG] Menu text detected in content")
        return False
    
    # ë‚´ìš© ê¸¸ì´ ë° êµ¬ì¡° í™•ì¸ - ë” ê´€ëŒ€í•˜ê²Œ
    sentences = content.split('.')
    valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
    
    if len(valid_sentences) < 2:
        print(f"[DEBUG] Insufficient valid sentences: {len(valid_sentences)}")
        return False
    
    # ë‹¨ì–´ ìˆ˜ í™•ì¸ - ë” ê´€ëŒ€í•˜ê²Œ (20ë‹¨ì–´ ì´ìƒ)
    words = content.split()
    if len(words) < 20:
        print(f"[DEBUG] Content too short: {len(words)} words")
        return False
    
    print(f"[DEBUG] Article validation passed - news_score: {news_score}, sentences: {len(valid_sentences)}, words: {len(words)}")
    return True

def extract_article_content_straits_times(url, soup):
    """The Straits Times ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1.headline, h1[data-testid="headline"], .article-headline h1, h1')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ì „ì²´ ë¹„í•„ìš” ìš”ì†Œ ë¨¼ì € ì œê±°
    remove_unwanted_elements(soup)
    
    # ë³¸ë¬¸ ì¶”ì¶œ - ë” ì •êµí•œ ì„ íƒì ì‚¬ìš©
    content_elem = find_main_content_element(soup, [
        'div[data-testid="article-body"]',
        '.article-content',
        '.paywall-content', 
        '.story-content',
        'article',
        '.content-body'
    ])
    
    if content_elem:
        article['content'] = extract_pure_article_text(content_elem)
    
    # ë‚ ì§œ ì¶”ì¶œ
    date_elem = soup.select_one('time, .published-date, [data-testid="publish-date"]')
    if date_elem:
        try:
            if date_elem.get('datetime'):
                article['publish_date'] = datetime.fromisoformat(date_elem['datetime'].replace('Z', '+00:00'))
            else:
                article['publish_date'] = datetime.now()
        except:
            article['publish_date'] = datetime.now()
    
    return article

def extract_article_content_moe(url, soup):
    """Ministry of Education ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1, .page-title, .content-title')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ë³¸ë¬¸ ì¶”ì¶œ - MOEëŠ” ì£¼ë¡œ div.content-area ì‚¬ìš©
    content_elem = soup.select_one('.content-area, .page-content, main')
    if content_elem:
        # ë„¤ë¹„ê²Œì´ì…˜, í—¤ë”, í‘¸í„° ì œê±°
        for elem in content_elem.select('nav, header, footer, .breadcrumb, .sidebar'):
            elem.decompose()
        
        # ì‹¤ì œ ì½˜í…ì¸  ì¶”ì¶œ
        paragraphs = content_elem.find_all(['p', 'li'])
        content_parts = []
        for p in paragraphs:
            text = clean_text(p.get_text())
            if len(text) > 20:  # ì§§ì€ í…ìŠ¤íŠ¸ í•„í„°ë§
                content_parts.append(text)
        
        article['content'] = ' '.join(content_parts[:10])[:1000]
    
    # ë‚ ì§œ ì¶”ì¶œ
    date_text = soup.get_text()
    date_match = re.search(r'(\d{1,2})\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(\d{4})', date_text)
    if date_match:
        try:
            date_str = f"{date_match.group(1)} {date_match.group(2)} {date_match.group(3)}"
            article['publish_date'] = datetime.strptime(date_str, "%d %b %Y")
        except:
            pass
    
    return article

def extract_article_content_nac(url, soup):
    """National Arts Council ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # NACëŠ” ì£¼ë¡œ ì´ë²¤íŠ¸ ì •ë³´
    title_elem = soup.select_one('h1, .event-title, .programme-title')
    if title_elem:
        article['title'] = clean_text(title_elem.get_text())
    
    # ì´ë²¤íŠ¸ ì„¤ëª… ì¶”ì¶œ
    content_elem = soup.select_one('.event-description, .programme-description, .content-main')
    if content_elem:
        article['content'] = clean_text(content_elem.get_text())[:1000]
    
    # ë‚ ì§œëŠ” ì´ë²¤íŠ¸ ë‚ ì§œë¡œ
    date_elem = soup.select_one('.event-date, .programme-date')
    if date_elem:
        article['publish_date'] = datetime.now()  # ê°„ë‹¨íˆ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
    
    return article

def extract_article_content_business_times(url, soup):
    """The Business Times ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1, .headline, .article-title')
    if title_elem:
        title_text = clean_text(title_elem.get_text())
        # ì‚¬ì´íŠ¸ ì´ë¦„ ì œê±°
        if ' - ' in title_text:
            title_text = title_text.split(' - ')[0]
        article['title'] = title_text
    
    # ì „ì²´ ë¹„í•„ìš” ìš”ì†Œ ë¨¼ì € ì œê±°
    remove_unwanted_elements(soup)
    
    # ë³¸ë¬¸ ì¶”ì¶œ - Business Times ì „ìš© ì„ íƒì
    content_elem = find_main_content_element(soup, [
        '.article-content',
        '.story-content',
        '.content-body',
        'article',
        '.post-content',
        '.main-content'
    ])
    
    if content_elem:
        article['content'] = extract_pure_article_text(content_elem)
    
    return article

def extract_article_content_cna(url, soup):
    """Channel NewsAsia ì „ìš© ì½˜í…ì¸  ì¶”ì¶œ"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.select_one('h1, .headline, .article-headline')
    if title_elem:
        title_text = clean_text(title_elem.get_text())
        # ì‚¬ì´íŠ¸ ì´ë¦„ ì œê±°
        if ' - ' in title_text:
            title_text = title_text.split(' - ')[0]
        article['title'] = title_text
    
    # ì „ì²´ ë¹„í•„ìš” ìš”ì†Œ ë¨¼ì € ì œê±°
    remove_unwanted_elements(soup)
    
    # ë³¸ë¬¸ ì¶”ì¶œ - CNA ì „ìš© ì„ íƒì
    content_elem = find_main_content_element(soup, [
        '.text-long',
        '.article-content',
        '.story-content',
        'article',
        '.content-body',
        '.post-content'
    ])
    
    if content_elem:
        article['content'] = extract_pure_article_text(content_elem)
    
    return article

def is_menu_text(text):
    """ë©”ë‰´ë‚˜ ë„¤ë¹„ê²Œì´ì…˜ í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸"""
    menu_indicators = [
        # ë¡œê·¸ì¸/ê³„ì • ê´€ë ¨
        'log in', 'sign in', 'account', 'my feed', 'manage account', 'log out',
        'subscribe', 'newsletter', 'subscription',
        
        # ë©”ë‰´/ë„¤ë¹„ê²Œì´ì…˜
        'menu', 'search', 'share', 'edition', 'search menu', 'breadcrumb',
        'navigation', 'header', 'footer', 'sidebar',
        
        # ì‚¬ì´íŠ¸ ì„¹ì…˜
        'top stories', 'latest news', 'breaking news', 'live tv', 'podcasts', 
        'radio schedule', 'tv schedule', 'watch', 'listen',
        
        # ì¹´í…Œê³ ë¦¬
        'business', 'sport', 'lifestyle', 'luxury', 'commentary', 'sustainability',
        'singapore', 'asia', 'world', 'insider', 'cna explains',
        
        # ê¸°ìˆ ì  ìš”ì†Œ
        'news id', 'type landing_page', 'type all_videos', 'type all_vod', 'type all_podcasts',
        'id 1822271', 'id 1821936', 'id 1821901', 'id 4310561', 'id 1821876',
        'id 1821886', 'id 1821896', 'id 3384986', 'id 1881506', 'id 1821906',
        'id 1821911', 'id 1821891', 'id 1431321', 'id 1822266', 'id 5197731',
        'id 2005266', 'id 5191361',
        
        # ê¸°íƒ€ UI ìš”ì†Œ
        'find out what', 'submitted by', 'anonymous', 'verified', 'newsletters',
        'get the best', 'select your', 'sent to your inbox', 'east asia',
        'us/uk', 'cnar', 'cna938', 'documentaries & shows', 'news reports'
    ]
    
    text_lower = text.lower().strip()
    
    # ì§ì ‘ ë§¤ì¹­
    if any(indicator in text_lower for indicator in menu_indicators):
        return True
    
    # íŒ¨í„´ ë§¤ì¹­
    import re
    
    # ID íŒ¨í„´ (News Id 1234567, Type landing_page ë“±)
    if re.search(r'\b(id|type)\s+\d+|\b(id|type)\s+\w+_\w+', text_lower):
        return True
    
    # ì§„ì—„í•œ ë©”ë‰´ íŒ¨í„´ (A B C D E... ë‚˜ì—´)
    words = text_lower.split()
    if len(words) > 10 and len([w for w in words if len(w) < 4]) > 5:
        return True
    
    # ì§„ì—„í•œ ì„ íƒìë‚˜ ë§í¬ ë‚˜ì—´
    if text_lower.count(':') > 3 or text_lower.count('|') > 2:
        return True
    
    # ì§„ì—„í•œ ì†Œë¬¸ì ë‚˜ì—´ (a b c d e f g...)
    if len(text) > 100 and len([c for c in text if c.isupper()]) < len(text) * 0.1:
        single_chars = [w for w in words if len(w) == 1]
        if len(single_chars) > 5:
            return True
    
    return False

def remove_unwanted_elements(soup):
    """ì „ì²´ í˜ì´ì§€ì—ì„œ ë¹„í•„ìš” ìš”ì†Œ ì œê±°"""
    # ì œê±°í•  ì„ íƒì ëª©ë¡
    unwanted_selectors = [
        # ë„¤ë¹„ê²Œì´ì…˜
        'nav', '.nav', '.navigation', '.navbar', '.menu', '.breadcrumb',
        # í—¤ë”/í‘¸í„°
        'header', 'footer', '.header', '.footer', '.page-header', '.page-footer',
        # ì‚¬ì´ë“œë°”
        '.sidebar', '.side-bar', '.left-sidebar', '.right-sidebar', 'aside',
        # ì†Œì…œ/ê³µìœ 
        '.social-share', '.share-buttons', '.social-links', '.social-media',
        # ë©”íƒ€ë°ì´í„°
        '.tags', '.tag-list', '.categories', '.meta', '.author-info',
        # ëŒ“ê¸€
        '.comments', '.comment-section', '.discussion',
        # ê´‘ê³ 
        '.advertisement', '.ads', '.banner', '.promo',
        # ê¸°íƒ€
        'script', 'style', '.hidden', '.sr-only',
        # CNA ì „ìš©
        '.c-header', '.c-footer', '.c-nav', '.c-sidebar',
        # ì¼ë°˜ì ì¸ ë©”ë‰´ í´ë˜ìŠ¤
        '.main-nav', '.primary-nav', '.secondary-nav'
    ]
    
    for selector in unwanted_selectors:
        for elem in soup.select(selector):
            elem.decompose()

def find_main_content_element(soup, selectors):
    """ì£¼ ì½˜í…ì¸  ìš”ì†Œ ì°¾ê¸°"""
    for selector in selectors:
        elem = soup.select_one(selector)
        if elem:
            return elem
    return None

def extract_pure_article_text(content_elem):
    """ìˆœìˆ˜ ê¸°ì‚¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ - ì¤‘ë³µ ì œê±° ë° ê°œì„ ëœ ì²˜ë¦¬"""
    # ë‚´ë¶€ì—ì„œ ì¶”ê°€ ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
    unwanted_inner = [
        '.related-articles', '.related-content', '.see-also',
        '.advertisement', '.ads', '.banner', '.promo',
        '.social-share', '.share-buttons', '.tags', '.meta',
        '.author-bio', '.author-info', '.byline',
        '.comments', '.comment-form', '.discussion',
        '.newsletter-signup', '.subscription',
        '.breadcrumb', '.navigation',
        'script', 'style', 'noscript'
    ]
    
    for selector in unwanted_inner:
        for elem in content_elem.select(selector):
            elem.decompose()
    
    # ë‹¨ë½ ì¶”ì¶œ - p íƒœê·¸ ìš°ì„ , divëŠ” pê°€ ì—†ì„ ë•Œë§Œ
    paragraphs = content_elem.find_all('p')
    if not paragraphs:
        paragraphs = content_elem.find_all('div')
    
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì¤‘ë³µ ì œê±°
    all_sentences = []
    seen_sentences = set()
    
    for p in paragraphs:
        text = clean_text(p.get_text())
        
        # ì§„ì§œ ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ í™•ì¸
        if not is_real_article_content(text):
            continue
            
        # ë©”ë‰´ í…ìŠ¤íŠ¸ ì œê±°
        text = TextProcessor.clean_menu_text(text)
        if not text:
            continue
            
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        sentences = TextProcessor.extract_sentences(text)
        
        for sentence in sentences:
            # ì •ê·œí™”ëœ ë¬¸ì¥ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            normalized = sentence.lower().strip()
            if normalized not in seen_sentences and len(sentence) > 20:
                seen_sentences.add(normalized)
                all_sentences.append(sentence)
    
    # ë¬¸ì¥ë“¤ì„ ë³‘í•©í•˜ë˜ ì•ˆì „í•˜ê²Œ ìë¥´ê¸°
    if all_sentences:
        merged_content = TextProcessor.merge_paragraphs(all_sentences, max_length=1000)
        return merged_content
    
    return ''

def is_real_article_content(text):
    """ì§„ì§œ ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ ì—„ê²©í•˜ê²Œ íŒë‹¨"""
    if len(text) < 30:  # ìµœì†Œ ê¸¸ì´ ì¦ê°€
        return False
    
    # ë©”ë‰´ í…ìŠ¤íŠ¸ ì²´í¬
    if is_menu_sentence(text):
        return False
    
    # ì§„ì§œ ê¸°ì‚¬ ë‚´ìš©ì˜ ì§€í‘œ
    article_signals = [
        # ì¸ìš©êµ¬
        'said', 'announced', 'reported', 'stated', 'explained', 'confirmed',
        'according to', 'in a statement', 'speaking to', 'told reporters',
        
        # ì •ë¶€/ê¸°ê´€
        'government', 'ministry', 'minister', 'prime minister', 'parliament',
        'official', 'spokesperson', 'department', 'agency',
        
        # ì§€ì—­/êµ­ê°€
        'singapore', 'malaysian', 'indonesian', 'thai', 'asean',
        
        # ìˆ˜ì¹˜/ë‚ ì§œ
        'percent', 'million', 'billion', 'year', 'month', 'week',
        'january', 'february', 'march', 'april', 'may', 'june',
        'july', 'august', 'september', 'october', 'november', 'december',
        
        # ê¸°ì‚¬ ë‚´ìš©
        'policy', 'economic', 'growth', 'development', 'investment',
        'business', 'market', 'industry', 'company', 'project'
    ]
    
    text_lower = text.lower()
    signal_count = sum(1 for signal in article_signals if signal in text_lower)
    
    # ê¸°ì‚¬ ì§€í‘œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¬¸ì¥ êµ¬ì¡° ì²´í¬
    if signal_count == 0:
        # ì™„ì „í•œ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ê°€ì§„ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        sentences = text.split('.')
        complete_sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        
        if len(complete_sentences) < 2:  # ìµœì†Œ 2ê°œ ë¬¸ì¥ í•„ìš”
            return False
        
        # ë‹¨ì–´ ê¸¸ì´ ë° ë¶„í¬ ì²´í¬
        words = text_lower.split()
        long_words = [w for w in words if len(w) > 4]
        
        if len(words) == 0 or len(long_words) / len(words) < 0.3:
            return False
    
    return signal_count > 0 or len(text.split('.')) >= 2

def extract_article_content_generic(url, soup):
    """ë²”ìš© ì½˜í…ì¸  ì¶”ì¶œ (í´ë°±)"""
    article = {
        'title': '',
        'content': '',
        'publish_date': datetime.now()
    }
    
    # ì œëª© ì¶”ì¶œ
    title_elem = soup.find('h1')
    if not title_elem:
        title_elem = soup.find('title')
    if title_elem:
        title_text = clean_text(title_elem.get_text())
        # ì‚¬ì´íŠ¸ ì´ë¦„ ì œê±°
        if ' - ' in title_text:
            title_text = title_text.split(' - ')[0]
        article['title'] = title_text
    
    # ì „ì²´ ë¹„í•„ìš” ìš”ì†Œ ë¨¼ì € ì œê±°
    remove_unwanted_elements(soup)
    
    # ë³¸ë¬¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
    content_elem = find_main_content_element(soup, [
        'article',
        'main',
        '.article-content',
        '.post-content',
        '.content-body',
        '.story-content',
        '.entry-content',
        '.main-content'
    ])
    
    if content_elem:
        article['content'] = extract_pure_article_text(content_elem)
    else:
        # í´ë°±: ê°€ì¥ ë§ì€ p íƒœê·¸ë¥¼ ê°€ì§„ div ì°¾ê¸°
        best_div = None
        max_content_score = 0
        
        for div in soup.find_all('div'):
            paragraphs = div.find_all('p')
            content_score = 0
            
            for p in paragraphs:
                text = clean_text(p.get_text())
                if is_real_article_content(text):
                    content_score += len(text)
            
            if content_score > max_content_score:
                max_content_score = content_score
                best_div = div
        
        if best_div:
            article['content'] = extract_pure_article_text(best_div)
    
    return article

def extract_article_content(url):
    """URLì— ë”°ë¼ ì ì ˆí•œ ì¶”ì¶œ ë°©ë²• ì„ íƒ"""
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë„ë©”ì¸ì— ë”°ë¼ ë‹¤ë¥¸ ì¶”ì¶œ ë°©ë²• ì‚¬ìš©
        domain = urlparse(url).netloc.lower()
        
        article_data = None
        if 'straitstimes.com' in domain:
            article_data = extract_article_content_straits_times(url, soup)
        elif 'businesstimes.com.sg' in domain:
            article_data = extract_article_content_business_times(url, soup)
        elif 'channelnewsasia.com' in domain or 'cna.com.sg' in domain:
            article_data = extract_article_content_cna(url, soup)
        elif 'moe.gov.sg' in domain:
            article_data = extract_article_content_moe(url, soup)
        elif 'nac.gov.sg' in domain or 'catch.sg' in domain:
            article_data = extract_article_content_nac(url, soup)
        else:
            article_data = extract_article_content_generic(url, soup)
        
        # ì¶”ì¶œëœ ë°ì´í„° í›„ì²˜ë¦¬
        if article_data:
            article_data = post_process_article_content(article_data)
        
        return article_data
            
    except Exception as e:
        print(f"Error extracting content from {url}: {e}")
        return None

def is_valid_article_url(url, domain):
    """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ íŒë‹¨ - ë” ìœ ì—°í•œ ì ‘ê·¼"""
    url_lower = url.lower()
    # print(f"[DEBUG] Checking URL: {url}")  # ë„ˆë¬´ ë§ì€ ì¶œë ¥ ë°©ì§€
    
    # ì œì™¸í•  íŒ¨í„´ë“¤ - í•µì‹¬ì ì¸ ê²ƒë“¤ë§Œ
    exclude_patterns = [
        'javascript:', 'mailto:', 'tel:', '#', 'wa.me', 'whatsapp',
        '.pdf', '.jpg', '.png', '.gif', '.mp4', '.css', '.js',
        'subscribe', 'login', 'register', 'sign-up', 'newsletter-signup',
        'privacy-policy', 'terms-of-service', 'contact-us', 'about-us',
        '/search', '/tag/', '/topic/', '/category/', '/author/'
    ]
    
    # ì œì™¸ íŒ¨í„´ ì²´í¬
    if any(pattern in url_lower for pattern in exclude_patterns):
        print(f"[DEBUG] URL excluded by pattern: {url}") 
        return False
    
    # ì‚¬ì´íŠ¸ë³„ ê¸°ì‚¬ URL íŒ¨í„´ - ë” ìœ ì—°í•˜ê²Œ
    if 'channelnewsasia.com' in domain or 'cna.com.sg' in domain:
        print(f"[DEBUG] Checking CNA URL patterns for: {url}")
        
        # CNA ê¸°ì‚¬ íŒ¨í„´ - ë” ìœ ì—°í•˜ê²Œ
        cna_patterns = [
            r'/singapore/[a-z0-9-]{5,}',          # ì‹±ê°€í¬ë¥´ ì„¹ì…˜ + ì§§ì€ ì œëª©ë„ í—ˆìš©
            r'/asia/[a-z0-9-]{5,}',               # ì•„ì‹œì•„ ì„¹ì…˜
            r'/world/[a-z0-9-]{5,}',              # ì›”ë“œ ì„¹ì…˜
            r'/business/[a-z0-9-]{5,}',           # ë¹„ì¦ˆë‹ˆìŠ¤ ì„¹ì…˜
            r'/sport/[a-z0-9-]{5,}',              # ìŠ¤í¬ì¸  ì„¹ì…˜
            r'/lifestyle/[a-z0-9-]{5,}',          # ë¼ì´í”„ìŠ¤íƒ€ì¼ ì„¹ì…˜
            r'/commentary/[a-z0-9-]{5,}',         # ë…¼í‰ ì„¹ì…˜
            r'/\d{4}/\d{2}/\d{2}/[a-z0-9-]{5,}', # ë‚ ì§œ íŒ¨í„´
            r'/[a-z0-9-]{10,}-\d+$'               # ê¸´ ì œëª©-ìˆ«ì íŒ¨í„´
        ]
        
        # ì„¹ì…˜ ë£¨íŠ¸ í˜ì´ì§€ëŠ” ì œì™¸
        if url.rstrip('/').endswith(('/singapore', '/asia', '/world', '/business', '/sport', '/lifestyle')):
            print(f"[DEBUG] CNA section root page excluded")
            return False
        
        matched = any(re.search(pattern, url) for pattern in cna_patterns)
        print(f"[DEBUG] CNA URL pattern match: {matched}")
        return matched
    
    elif 'straitstimes.com' in domain:
        # print(f"[DEBUG] Checking ST URL patterns for: {url}")  # ë„ˆë¬´ ë§ì€ ì¶œë ¥ ë°©ì§€
        
        # Straits Times ê¸°ì‚¬ íŒ¨í„´ - ëŒ€ì†Œë¬¸ì ëª¨ë‘ í—ˆìš©í•˜ê³  ë” ìœ ì—°í•˜ê²Œ
        st_patterns = [
            r'/singapore/[a-zA-Z0-9-]{5,}',          # ì‹±ê°€í¬ë¥´ ì„¹ì…˜
            r'/asia/[a-zA-Z0-9-/]{5,}',              # ì•„ì‹œì•„ ì„¹ì…˜
            r'/world/[a-zA-Z0-9-/]{5,}',             # ì›”ë“œ ì„¹ì…˜
            r'/business/[a-zA-Z0-9-/]{5,}',          # ë¹„ì¦ˆë‹ˆìŠ¤ ì„¹ì…˜
            r'/sport/[a-zA-Z0-9-/]{5,}',             # ìŠ¤í¬ì¸  ì„¹ì…˜
            r'/life/[a-zA-Z0-9-/]{5,}',              # ë¼ì´í”„ ì„¹ì…˜
            r'/opinion/[a-zA-Z0-9-]{5,}',            # ì˜¤í”¼ë‹ˆì–¸ ì„¹ì…˜
            r'/tech/[a-zA-Z0-9-]{5,}',               # ê¸°ìˆ  ì„¹ì…˜
            r'/politics/[a-zA-Z0-9-]{5,}',           # ì •ì¹˜ ì„¹ì…˜
            r'/\d{4}/\d{2}/\d{2}/[a-zA-Z0-9-]{5,}', # ë‚ ì§œ íŒ¨í„´
            r'/[a-zA-Z]{2,}/[a-zA-Z0-9-]{5,}',       # ë‘ ê¸€ì ì„¹ì…˜
            r'/[a-zA-Z0-9-]{10,}'                     # ì¼ë°˜ ê¸°ì‚¬ íŒ¨í„´ (ë” ìœ ì—°í•˜ê²Œ)
        ]
        
        # ST íŠ¹ë³„ í˜ì´ì§€ë“¤ë§Œ ì œì™¸
        if any(exclude in url_lower for exclude in ['/multimedia/', '/graphics/', '/180']):
            print(f"[DEBUG] ST special page excluded")
            return False
            
        matched = any(re.search(pattern, url) for pattern in st_patterns)
        print(f"[DEBUG] ST URL pattern match: {matched}")
        return matched
    
    elif 'businesstimes.com.sg' in domain:
        print(f"[DEBUG] Checking BT URL patterns for: {url}")
        
        # Business Times ê¸°ì‚¬ íŒ¨í„´ - ë” ìœ ì—°í•˜ê²Œ
        bt_patterns = [
            r'/economy/[a-z0-9-]{5,}',            # ê²½ì œ ì„¹ì…˜
            r'/companies/[a-z0-9-]{5,}',          # ê¸°ì—… ì„¹ì…˜
            r'/banking-finance/[a-z0-9-]{5,}',    # ê¸ˆìœµ ì„¹ì…˜
            r'/asean/[a-z0-9-]{5,}',              # ì•„ì„¸ì•ˆ ì„¹ì…˜
            r'/tech/[a-z0-9-]{5,}',               # ê¸°ìˆ  ì„¹ì…˜
            r'/\d{4}/\d{2}/\d{2}/[a-z0-9-]{5,}', # ë‚ ì§œ íŒ¨í„´
            r'/[a-z0-9-]{15,}$'                   # ê¸´ ì œëª© URL
        ]
        
        matched = any(re.search(pattern, url) for pattern in bt_patterns)
        print(f"[DEBUG] BT URL pattern match: {matched}")
        return matched
    
    elif 'sbr.com.sg' in domain:
        print(f"[DEBUG] Checking SBR URL patterns for: {url}")
        
        # Singapore Business Review ê¸°ì‚¬ íŒ¨í„´
        sbr_patterns = [
            r'/economy/[a-z0-9-]{5,}',                # ê²½ì œ ì„¹ì…˜
            r'/companies/[a-z0-9-]{5,}',              # ê¸°ì—… ì„¹ì…˜
            r'/banking/[a-z0-9-]{5,}',                # ê¸ˆìœµ ì„¹ì…˜
            r'/real-estate/[a-z0-9-]{5,}',            # ë¶€ë™ì‚° ì„¹ì…˜
            r'/technology/[a-z0-9-]{5,}',             # ê¸°ìˆ  ì„¹ì…˜
            r'/startups/[a-z0-9-]{5,}',               # ìŠ¤íƒ€íŠ¸ì—… ì„¹ì…˜
            r'/sustainability/[a-z0-9-]{5,}',         # ì§€ì†ê°€ëŠ¥ì„± ì„¹ì…˜
            r'/\d{4}/\d{2}/\d{2}/[a-z0-9-]{5,}',     # ë‚ ì§œ íŒ¨í„´
            r'/[a-z0-9-]{10,}$'                       # ê¸°ì‚¬ ì œëª© URL
        ]
        
        matched = any(re.search(pattern, url) for pattern in sbr_patterns)
        print(f"[DEBUG] SBR URL pattern match: {matched}")
        return matched
    
    elif 'moe.gov.sg' in domain:
        print(f"[DEBUG] Checking MOE URL patterns for: {url}")
        
        # Ministry of Education í˜ì´ì§€ íŒ¨í„´
        moe_patterns = [
            r'/news/[a-z0-9-]{5,}',                   # ë‰´ìŠ¤
            r'/press-releases/[a-z0-9-]{5,}',         # ë³´ë„ìë£Œ
            r'/parliamentary-replies/[a-z0-9-]{5,}',  # êµ­íšŒ ë‹µë³€
            r'/speeches/[a-z0-9-]{5,}',               # ì—°ì„¤
            r'/initiatives/[a-z0-9-]{5,}',            # ì´ë‹ˆì…”í‹°ë¸Œ
            r'/policies/[a-z0-9-]{5,}',               # ì •ì±…
            r'/programmes/[a-z0-9-]{5,}',             # í”„ë¡œê·¸ë¨
            r'/\d{4}/\d{2}/\d{2}/[a-z0-9-]{5,}',     # ë‚ ì§œ íŒ¨í„´
            r'/[a-z0-9-]{10,}$'                       # ê¸´ ì œëª© URL
        ]
        
        matched = any(re.search(pattern, url) for pattern in moe_patterns)
        print(f"[DEBUG] MOE URL pattern match: {matched}")
        return matched
    
    elif 'nac.gov.sg' in domain:
        print(f"[DEBUG] Checking NAC URL patterns for: {url}")
        
        # National Arts Council í˜ì´ì§€ íŒ¨í„´
        nac_patterns = [
            r'/whatson/[a-z0-9-]{5,}',                # í–‰ì‚¬/í”„ë¡œê·¸ë¨
            r'/engage/[a-z0-9-]{5,}',                 # ì°¸ì—¬ í”„ë¡œê·¸ë¨
            r'/news/[a-z0-9-]{5,}',                   # ë‰´ìŠ¤
            r'/press-releases/[a-z0-9-]{5,}',         # ë³´ë„ìë£Œ
            r'/events/[a-z0-9-]{5,}',                 # ì´ë²¤íŠ¸
            r'/programmes/[a-z0-9-]{5,}',             # í”„ë¡œê·¸ë¨
            r'/grants/[a-z0-9-]{5,}',                 # ë³´ì¡°ê¸ˆ ì •ë³´
            r'/initiatives/[a-z0-9-]{5,}',            # ì´ë‹ˆì…”í‹°ë¸Œ
            r'/\d{4}/\d{2}/\d{2}/[a-z0-9-]{5,}',     # ë‚ ì§œ íŒ¨í„´
            r'/[a-z0-9-]{10,}$'                       # ê¸´ ì œëª© URL
        ]
        
        matched = any(re.search(pattern, url) for pattern in nac_patterns)
        print(f"[DEBUG] NAC URL pattern match: {matched}")
        return matched
    
    # ê¸°ë³¸ íŒ¨í„´ (ëª¨ë“  ì‚¬ì´íŠ¸ìš©) - ë§¤ìš° ê´€ëŒ€í•˜ê²Œ
    general_patterns = [
        r'/20\d{2}/\d{2}/\d{2}/[a-z0-9-]{3,}',   # ë‚ ì§œ + ì œëª© íŒ¨í„´ (ë” ì§§ì€ ì œëª©ë„ í—ˆìš©)
        r'/articles?/[a-z0-9-]{3,}',             # ê¸°ì‚¬ URL
        r'/news/[a-z0-9-]{3,}',                  # ë‰´ìŠ¤ URL
        r'/story/[a-z0-9-]{3,}',                 # ìŠ¤í† ë¦¬ URL
        r'/post/[a-z0-9-]{3,}',                  # í¬ìŠ¤íŠ¸ URL
        r'/press-releases/[a-z0-9-]{3,}',        # ë³´ë„ìë£Œ URL
        r'/events?/[a-z0-9-]{3,}',               # ì´ë²¤íŠ¸ URL
        r'/programmes?/[a-z0-9-]{3,}',           # í”„ë¡œê·¸ë¨ URL
        r'/initiatives?/[a-z0-9-]{3,}',          # ì´ë‹ˆì…”í‹°ë¸Œ URL
        r'/policies/[a-z0-9-]{3,}',              # ì •ì±… URL
        r'/speeches/[a-z0-9-]{3,}',              # ì—°ì„¤ URL
        r'/singapore/[a-z0-9-]{3,}',             # ì‹±ê°€í¬ë¥´ ì„¹ì…˜
        r'/asia/[a-z0-9-]{3,}',                  # ì•„ì‹œì•„ ì„¹ì…˜
        r'/world/[a-z0-9-]{3,}',                 # ì›”ë“œ ì„¹ì…˜
        r'/business/[a-z0-9-]{3,}',              # ë¹„ì¦ˆë‹ˆìŠ¤ ì„¹ì…˜
        r'/economy/[a-z0-9-]{3,}',               # ê²½ì œ ì„¹ì…˜
        r'/technology/[a-z0-9-]{3,}',            # ê¸°ìˆ  ì„¹ì…˜
        r'/[a-z0-9-]{10,}$',                     # ê¸´ ì œëª© URL (10ì ì´ìƒ)
        r'/[a-z0-9-]+-\d+$',                     # ì œëª©-ìˆ«ì íŒ¨í„´
        r'/[a-z0-9-]+/[a-z0-9-]{5,}',           # ì¹´í…Œê³ ë¦¬/ì œëª© íŒ¨í„´
        r'\w+://[^/]+/[^?#]+[a-zA-Z0-9-]{5,}'   # ì¼ë°˜ì ì¸ ì½˜í…ì¸  URL (ì¿¼ë¦¬/ì•µì»¤ ì œì™¸)
    ]
    
    # URLì´ ìµœì†Œ ê¸¸ì´ ìš”ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
    if len(url) < 30:  # ë„ˆë¬´ ì§§ì€ URLì€ ê¸°ì‚¬ê°€ ì•„ë‹ ê°€ëŠ¥ì„±
        print(f"[DEBUG] URL too short: {url}")
        return False
    
    matched = any(re.search(pattern, url) for pattern in general_patterns)
    print(f"[DEBUG] General URL pattern match: {matched} for {url}")
    return matched

def get_article_links_straits_times(soup, base_url):
    """The Straits Times ì „ìš© ë§í¬ ì¶”ì¶œ"""
    links = []
    domain = urlparse(base_url).netloc.lower()
    
    for a in soup.select('a[href]'):
        href = a.get('href', '')
        full_url = urljoin(base_url, href)
        
        # ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸
        if is_valid_article_url(full_url, domain):
            links.append(full_url)
    
    return list(set(links))[:10]  # ì¤‘ë³µ ì œê±° í›„ 10ê°œê¹Œì§€

def get_article_links_moe(soup, base_url):
    """MOE ì „ìš© ë§í¬ ì¶”ì¶œ"""
    links = []
    domain = urlparse(base_url).netloc.lower()
    
    # MOEëŠ” ì£¼ë¡œ press-releasesì™€ news ì„¹ì…˜
    for a in soup.select('a[href*="press-releases"], a[href*="/news/"]'):
        href = a.get('href', '')
        full_url = urljoin(base_url, href)
        
        # ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸
        if is_valid_article_url(full_url, domain):
            links.append(full_url)
    
    return list(set(links))[:10]

def get_article_links_generic(soup, base_url):
    """ë²”ìš© ë§í¬ ì¶”ì¶œ - ê°œì„ ëœ ë²„ì „"""
    links = []
    domain = urlparse(base_url).netloc.lower()
    print(f"[DEBUG] Generic link extraction for domain: {domain}")
    
    # ëª¨ë“  ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ë˜, ë” ë˜‘ë˜‘í•œ í•„í„°ë§ ì ìš©
    all_links = []
    for a in soup.select('a[href]'):
        href = a.get('href', '')
        if not href:
            continue
            
        full_url = urljoin(base_url, href)
        link_text = a.get_text(strip=True)
        
        # ë§í¬ê°€ ê¸°ë³¸ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸
        if len(full_url) > 30 and link_text:  # ë§í¬ì™€ í…ìŠ¤íŠ¸ê°€ ìˆì–´ì•¼ í•¨
            all_links.append({
                'url': full_url,
                'text': link_text,
                'domain': urlparse(full_url).netloc.lower()
            })
    
    print(f"[DEBUG] Found {len(all_links)} total links")
    
    # ë„ë©”ì¸ ë‚´ ë§í¬ë§Œ í•„í„°ë§
    same_domain_links = [link for link in all_links if domain in link['domain']]
    print(f"[DEBUG] Found {len(same_domain_links)} same-domain links")
    
    # ê° ë§í¬ë¥¼ URL íŒ¨í„´ìœ¼ë¡œ ê²€ì¦
    for link in same_domain_links:
        if is_valid_article_url(link['url'], domain):
            links.append(link['url'])
            print(f"[DEBUG] Added valid link: {link['url']}")
            
            # ìµœëŒ€ 15ê°œê¹Œì§€ë§Œ ìˆ˜ì§‘
            if len(links) >= 15:
                break
    
    print(f"[DEBUG] Final link count for {domain}: {len(links)}")
    return list(set(links))[:10]  # ì¤‘ë³µ ì œê±° í›„ ìµœëŒ€ 10ê°œ

def create_summary(article_data, settings):
    """ì„¤ì •ì— ë”°ë¥¸ ìš”ì•½ ìƒì„±"""
    # ì„¤ì •ì—ì„œ AI ì˜µì…˜ í™•ì¸
    ai_options = settings.get('scrapingMethodOptions', {}).get('ai', {})
    provider = ai_options.get('provider', 'gemini')
    
    print(f"[SUMMARY] AI provider: {provider}")
    print(f"[SUMMARY] Gemini API key available: {bool(os.environ.get('GOOGLE_GEMINI_API_KEY'))}")
    
    # Gemini API ì‚¬ìš© ì‹œë„
    if provider == 'gemini' and os.environ.get('GOOGLE_GEMINI_API_KEY'):
        try:
            print(f"[SUMMARY] Attempting Gemini API translation for: {article_data['title'][:50]}...")
            from ai_summary_free import translate_to_korean_summary_gemini
            gemini_summary = translate_to_korean_summary_gemini(
                article_data['title'], 
                article_data['content']
            )
            if gemini_summary:
                print(f"[SUMMARY] Gemini API success: {gemini_summary[:100]}...")
                return gemini_summary
            else:
                print(f"[SUMMARY] Gemini API returned empty result")
        except Exception as e:
            print(f"[SUMMARY] Gemini API ì˜¤ë¥˜, í‚¤ì›Œë“œ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´: {str(e)}")
    else:
        print(f"[SUMMARY] Gemini API not available - provider: {provider}, key: {bool(os.environ.get('GOOGLE_GEMINI_API_KEY'))}")
    
    # Gemini ì‹¤íŒ¨ì‹œ í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½ ì‚¬ìš©
    print(f"[SUMMARY] Using enhanced keyword-based summary")
    try:
        from ai_summary_free import enhanced_keyword_summary
        return enhanced_keyword_summary(article_data['title'], article_data['content'])
    except Exception as e:
        print(f"[SUMMARY] Enhanced summary failed, using basic: {str(e)}")
        return create_keyword_summary(article_data['title'], article_data['content'])

def create_keyword_summary(title, content):
    """í–¥ìƒëœ í‚¤ì›Œë“œ ê¸°ë°˜ í•œê¸€ ìš”ì•½"""
    # ì •í™•í•œ í‚¤ì›Œë“œ ë§¤í•‘
    keywords = {
        # ê¸°ë³¸ í‚¤ì›Œë“œ
        'singapore': 'ì‹±ê°€í¬ë¥´', 'economy': 'ê²½ì œ', 'government': 'ì •ë¶€',
        'education': 'êµìœ¡', 'health': 'ë³´ê±´', 'transport': 'êµí†µ',
        'technology': 'ê¸°ìˆ ', 'business': 'ë¹„ì¦ˆë‹ˆìŠ¤', 'covid': 'ì½”ë¡œë‚˜',
        'minister': 'ì¥ê´€', 'policy': 'ì •ì±…', 'development': 'ê°œë°œ',
        
        # êµí†µ ê´€ë ¨
        'train': 'êµí†µ', 'mrt': 'êµí†µ', 'lrt': 'êµí†µ', 'bus': 'êµí†µ',
        'transport': 'êµí†µ', 'traffic': 'êµí†µ', 'airport': 'ê³µí•­', 'changi': 'ì°½ì´',
        
        # êµìœ¡ ë° ì‚¬íšŒ
        'school': 'êµìœ¡', 'student': 'êµìœ¡', 'university': 'êµìœ¡',
        'employment': 'ì·¨ì—…', 'job': 'ì·¨ì—…', 'work': 'ì·¨ì—…', 'salary': 'ì·¨ì—…',
        'mom': 'ì·¨ì—…', 'manpower': 'ì·¨ì—…', 'worker': 'ì·¨ì—…',
        
        # ë¶€ë™ì‚° ë° ì£¼íƒ
        'housing': 'ì£¼íƒ', 'hdb': 'ì£¼íƒ', 'condo': 'ë¶€ë™ì‚°',
        'property': 'ë¶€ë™ì‚°', 'condominium': 'ë¶€ë™ì‚°',
        
        # ë²•ë¥  ë° ë²”ì£„
        'police': 'ë²•ë¥ ', 'court': 'ë²•ë¥ ', 'law': 'ë²•ë¥ ', 'crime': 'ë²”ì£„',
        'jail': 'ë²”ì£„', 'sentenced': 'ë²•ë¥ ', 'trial': 'ë²•ë¥ ',
        
        # ê²½ì œ ë° ê¸ˆìœµ
        'market': 'ê²½ì œ', 'stock': 'ê²½ì œ', 'bank': 'ê¸ˆìœµ', 'finance': 'ê¸ˆìœµ',
        'investment': 'ê²½ì œ', 'trade': 'ê²½ì œ', 'gdp': 'ê²½ì œ',
        
        # ìŒì‹ ë° ë¬¸í™”
        'food': 'ìŒì‹', 'restaurant': 'ìŒì‹', 'hawker': 'ìŒì‹',
        'culture': 'ë¬¸í™”', 'arts': 'ë¬¸í™”', 'festival': 'ë¬¸í™”',
        
        # ì •ì¹˜ ë° ì„ ê±°
        'election': 'ì •ì¹˜', 'parliament': 'ì •ì¹˜', 'voting': 'ì„ ê±°',
        'prime': 'ì •ì¹˜', 'president': 'ì •ì¹˜',
        
        # í™˜ê²½ ë° ê¸°í›„
        'climate': 'í™˜ê²½', 'environment': 'í™˜ê²½', 'green': 'í™˜ê²½',
        'carbon': 'í™˜ê²½', 'sustainability': 'í™˜ê²½',
        
        # ê¸°ìˆ  ë° í˜ì‹ 
        'startup': 'ê¸°ìˆ ', 'innovation': 'ê¸°ìˆ ', 'digital': 'ê¸°ìˆ ',
        'ai': 'ê¸°ìˆ ', 'artificial': 'ê¸°ìˆ ', 'data': 'ê¸°ìˆ ', 'cyber': 'ê¸°ìˆ ',
        
        # ê´€ê´‘ ë° ì—¬í–‰
        'tourism': 'ê´€ê´‘', 'tourist': 'ê´€ê´‘', 'travel': 'ê´€ê´‘',
        'visitor': 'ê´€ê´‘', 'hotel': 'ê´€ê´‘',
        
        # êµ­ì œ ê´€ê³„
        'malaysia': 'êµ­ì œ', 'indonesia': 'êµ­ì œ', 'thailand': 'êµ­ì œ',
        'china': 'êµ­ì œ', 'india': 'êµ­ì œ', 'japan': 'êµ­ì œ', 'korea': 'êµ­ì œ',
        'asean': 'êµ­ì œ', 'asia': 'êµ­ì œ', 'global': 'êµ­ì œ'
    }
    
    # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    found_keywords = []
    text_lower = (title + ' ' + content).lower()
    
    for eng, kor in keywords.items():
        if eng in text_lower:
            found_keywords.append(kor)
    
    # ì¤‘ë³µ ì œê±°
    found_keywords = list(dict.fromkeys(found_keywords))
    
    # ìš”ì•½ ìƒì„±
    if found_keywords:
        summary = f"ğŸ“° {', '.join(found_keywords[:3])} ê´€ë ¨ ë‰´ìŠ¤"
    else:
        summary = f"ğŸ“° ì‹±ê°€í¬ë¥´ ìµœì‹  ë‰´ìŠ¤"
    
    # ì œëª©ë§Œ í¬í•¨ (ì›ë¬¸ ë‚´ìš© ì œì™¸)
    summary += f"\nğŸ“¢ {title[:80]}{'...' if len(title) > 80 else ''}"
    
    return summary

def scrape_news_ai():
    """AI ê¸°ë°˜ í–¥ìƒëœ ë‰´ìŠ¤ ìŠ¤í¬ë© í•¨ìˆ˜"""
    settings = load_settings()
    sites = load_sites()
    articles_by_group = defaultdict(list)
    
    blocked_keywords = [kw.strip() for kw in settings.get('blockedKeywords', '').split(',') if kw.strip()]
    important_keywords = [kw.strip() for kw in settings.get('importantKeywords', '').split(',') if kw.strip()]
    
    for site in sites:
        try:
            print(f"AI Scraping {site['name']}...")
            
            # AI ìŠ¤í¬ë˜í¼ë¡œ ì‚¬ì´íŠ¸ ë¶„ì„
            site_result = ai_scraper.scrape_with_ai(site['url'])
            
            if site_result['type'] == 'error':
                print(f"[ERROR] Failed to scrape {site['name']}: {site_result['error']}")
                continue
            
            # ë§í¬ í˜ì´ì§€ì¸ ê²½ìš° - ê¸°ì‚¬ ë§í¬ë“¤ ì¶”ì¶œ
            if site_result['type'] == 'link_page':
                links = site_result.get('links', [])
                print(f"[AI] Found {len(links)} article links from {site['name']}")
                
                # ê° ë§í¬ì— ëŒ€í•´ ê¸°ì‚¬ ì¶”ì¶œ
                for article_url in links[:5]:  # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 5ê°œ
                    try:
                        print(f"[AI] Processing article: {article_url}")
                        
                        # AIë¡œ ê¸°ì‚¬ ì¶”ì¶œ
                        article_result = ai_scraper.scrape_with_ai(article_url)
                        
                        if article_result['type'] != 'article':
                            print(f"[AI] Skipping: not an article - {article_result['type']}")
                            continue
                        
                        # ê¸°ì‚¬ ë°ì´í„° ê²€ì¦
                        if not article_result.get('title') or not article_result.get('content'):
                            print(f"[AI] Skipping: missing title or content")
                            continue
                        
                        if len(article_result['content']) < 50:
                            print(f"[AI] Skipping: content too short ({len(article_result['content'])} chars)")
                            continue
                        
                        # í‚¤ì›Œë“œ í•„í„°ë§
                        full_text = f"{article_result['title']} {article_result['content']}"
                        
                        if is_blocked(full_text, blocked_keywords):
                            print(f"[AI] Skipping: blocked by keywords")
                            continue
                        
                        if settings['scrapTarget'] == 'important' and not contains_keywords(full_text, important_keywords):
                            print(f"[AI] Skipping: no important keywords")
                            continue
                        
                        print(f"[AI] Article passed validation: {article_result['title']}")
                        
                        # ìš”ì•½ ìƒì„±
                        article_data = {
                            'title': article_result['title'],
                            'content': article_result['content'],
                            'publish_date': datetime.now()
                        }
                        summary = create_summary(article_data, settings)
                        print(f"[AI] Generated summary: {summary[:100]}...")
                        
                        # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
                        articles_by_group[site['group']].append({
                            'site': site['name'],
                            'title': article_result['title'],
                            'url': article_url,
                            'summary': summary,
                            'content': article_result['content'],
                            'publish_date': datetime.now().isoformat(),
                            'extracted_by': article_result.get('extracted_by', 'ai'),
                            'ai_classification': article_result.get('classification', {})
                        })
                        
                    except Exception as e:
                        print(f"[ERROR] Error processing article {article_url}: {e}")
                        continue
            
            # ì§ì ‘ ê¸°ì‚¬ì¸ ê²½ìš°
            elif site_result['type'] == 'article':
                print(f"[AI] Direct article found from {site['name']}")
                
                if site_result.get('title') and site_result.get('content'):
                    full_text = f"{site_result['title']} {site_result['content']}"
                    
                    # í•„í„°ë§
                    if not is_blocked(full_text, blocked_keywords):
                        if settings['scrapTarget'] != 'important' or contains_keywords(full_text, important_keywords):
                            
                            article_data = {
                                'title': site_result['title'],
                                'content': site_result['content'],
                                'publish_date': datetime.now()
                            }
                            summary = create_summary(article_data, settings)
                            
                            articles_by_group[site['group']].append({
                                'site': site['name'],
                                'title': site_result['title'],
                                'url': site['url'],
                                'summary': summary,
                                'content': site_result['content'],
                                'publish_date': datetime.now().isoformat(),
                                'extracted_by': site_result.get('extracted_by', 'ai'),
                                'ai_classification': site_result.get('classification', {})
                            })
                            
                            print(f"[AI] Direct article processed: {site_result['title']}")
                        else:
                            print(f"[AI] Direct article filtered by keywords")
                    else:
                        print(f"[AI] Direct article blocked by keywords")
                else:
                    print(f"[AI] Direct article missing title or content")
                    
        except Exception as e:
            print(f"[ERROR] Error scraping {site['name']}: {e}")
            continue
    
    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ í†µí•©
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
            
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in group_articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œì˜ ì£¼ìš” ê¸°ì‚¬ë§Œ ì„ íƒ
        selected_articles = unique_articles[:3]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': datetime.now().isoformat(),
            'scraping_method': 'ai_enhanced'
        }
        
        consolidated_articles.append(group_summary)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    # latest.json íŒŒì¼ ì—…ë°ì´íŠ¸
    latest_info = {
        'lastUpdated': datetime.now().isoformat(),
        'latestFile': f'news_{timestamp}.json',
        'scrapingMethod': 'ai_enhanced'
    }
    with open('data/latest.json', 'w', encoding='utf-8') as f:
        json.dump(latest_info, f, ensure_ascii=False, indent=2)
    
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    print(f"\n[AI] Scraped {total_articles} articles from {len(consolidated_articles)} groups")
    return output_file

def scrape_news():
    """ë©”ì¸ ìŠ¤í¬ë© í•¨ìˆ˜ - ì„¤ì •ì— ë”°ë¼ ë°©ì‹ ì„ íƒ"""
    settings = load_settings()
    scraping_method = settings.get('scrapingMethod', 'traditional')
    
    print(f"[SCRAPER] Selected method: {scraping_method}")
    print(f"[SCRAPER] AI model status: {ai_scraper.model is not None}")
    print(f"[SCRAPER] AI API key: {bool(ai_scraper.api_key)}")
    
    if scraping_method == 'ai':
        if ai_scraper.model:
            print("Using AI-enhanced scraping...")
            try:
                return scrape_news_ai()
            except Exception as e:
                print(f"AI scraping failed: {e}")
                if settings.get('scrapingMethodOptions', {}).get('ai', {}).get('fallbackToTraditional', True):
                    print("Falling back to traditional scraping with AI summaries...")
                    return scrape_news_traditional()
                else:
                    raise
        else:
            print("AI scraping requested but AI model not available. Using traditional scraping with AI summaries...")
            return scrape_news_traditional()
    else:
        print("Using traditional scraping...")
        return scrape_news_traditional()

def scrape_news_traditional():
    """ê¸°ì¡´ ë°©ì‹ì˜ ìŠ¤í¬ë© í•¨ìˆ˜ (AI ì—†ì´)"""
    settings = load_settings()
    sites = load_sites()
    articles_by_group = defaultdict(list)
    
    blocked_keywords = [kw.strip() for kw in settings.get('blockedKeywords', '').split(',') if kw.strip()]
    important_keywords = [kw.strip() for kw in settings.get('importantKeywords', '').split(',') if kw.strip()]
    
    for site in sites:
        try:
            print(f"\n[SCRAPER] === Scraping {site['name']} ({site['url']}) ===")
            response = requests.get(site['url'], timeout=10, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            print(f"[SCRAPER] HTTP Status: {response.status_code}")
            if response.status_code != 200:
                print(f"[SCRAPER] Failed to access {site['name']}: HTTP {response.status_code}")
                continue
                
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì‚¬ì´íŠ¸ë³„ ë§í¬ ì¶”ì¶œ
            domain = urlparse(site['url']).netloc.lower()
            print(f"[SCRAPER] Domain: {domain}")
            
            if 'straitstimes.com' in domain:
                print(f"[SCRAPER] Using Straits Times specific extractor")
                links = get_article_links_straits_times(soup, site['url'])
            elif 'moe.gov.sg' in domain:
                print(f"[SCRAPER] Using MOE specific extractor")
                links = get_article_links_moe(soup, site['url'])
            else:
                print(f"[SCRAPER] Using generic extractor")
                links = get_article_links_generic(soup, site['url'])
            
            print(f"[SCRAPER] Found {len(links)} article links for {site['name']}")
            if len(links) == 0:
                print(f"[SCRAPER] WARNING: No links found for {site['name']} - site may have changed structure")
                # í˜ì´ì§€ íƒ€ì´í‹€ í™•ì¸
                title = soup.title.string if soup.title else "No title"
                print(f"[SCRAPER] Page title: {title[:100]}")
                continue
            
            # ê¸°ì‚¬ë³„ ì²˜ë¦¬
            for article_url in links[:3]:  # ì‚¬ì´íŠ¸ë‹¹ ìµœëŒ€ 3ê°œë¡œ ì¶•ì†Œ (ì„±ëŠ¥ ê°œì„ )
                try:
                    if DEBUG_MODE:
                        print(f"[DEBUG] Processing article: {article_url}")
                    article_data = extract_article_content(article_url)
                    
                    if not article_data or not article_data['title']:
                        if DEBUG_MODE:
                            print(f"[DEBUG] Skipping: no title or data")
                        continue
                        
                    if len(article_data['content']) < 30:
                        print(f"[DEBUG] Skipping: content too short ({len(article_data['content'])} chars)")
                        continue
                    
                    # ì œëª©ë¶€í„° ë©”ë‰´/ë„¤ë¹„ê²Œì´ì…˜ í˜ì´ì§€ í™•ì¸
                    if is_menu_text(article_data['title']):
                        print(f"[DEBUG] Skipping: menu title detected - {article_data['title']}")
                        continue
                    
                    # ëœë”© í˜ì´ì§€ ë˜ëŠ” ë©”ë‰´ í˜ì´ì§€ì¸ì§€ í™•ì¸
                    if is_landing_page_content(article_data['content']):
                        print(f"[DEBUG] Skipping: landing page content detected")
                        continue
                        
                    # ì˜ë¯¸ìˆëŠ” ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ í™•ì¸
                    if not is_meaningful_content(article_data['content']):
                        print(f"[DEBUG] Skipping: not meaningful content")
                        continue
                    
                    # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ í•„í„°ë§ (ì œëª© ê¸°ë°˜)
                    category_page_titles = [
                        'features', 'big read', 'top stories', 'latest news',
                        'breaking news', 'world news', 'asia news', 'business news',
                        'opinion', 'lifestyle', 'sports', 'technology',
                        'property', 'investment', 'markets', 'commentary',
                        'learning minds', 'newsletter', 'subscribe', 'health',
                        'politics', 'science', 'culture', 'entertainment'
                    ]
                    
                    if any(cat.lower() == article_data['title'].lower().strip() for cat in category_page_titles):
                        print(f"[DEBUG] Skipping: category page title detected - {article_data['title']}")
                        continue
                    
                    full_text = f"{article_data['title']} {article_data['content']}"
                    
                    # í•„í„°ë§
                    if is_blocked(full_text, blocked_keywords):
                        print(f"[DEBUG] Skipping: blocked by keywords")
                        continue
                    
                    if settings['scrapTarget'] == 'recent' and not is_recent_article(article_data['publish_date']):
                        print(f"[DEBUG] Skipping: not recent article")
                        continue
                    
                    if settings['scrapTarget'] == 'important' and not contains_keywords(full_text, important_keywords):
                        print(f"[DEBUG] Skipping: no important keywords")
                        continue
                    
                    # ìµœì¢… ìœ íš¨ì„± ê²€ì‚¬ - ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì¸ì§€ ì¬í™•ì¸
                    if not validate_final_article_content(article_data):
                        print(f"[DEBUG] Skipping: failed final validation")
                        continue
                    
                    print(f"[DEBUG] Article passed all validations: {article_data['title']}")
                    
                    # ìš”ì•½ ìƒì„±
                    summary = create_summary(article_data, settings)
                    print(f"[DEBUG] Generated summary: {summary[:100]}...")
                    
                    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
                    articles_by_group[site['group']].append({
                        'site': site['name'],
                        'title': article_data['title'],
                        'url': article_url,
                        'summary': summary,
                        'content': article_data['content'],
                        'publish_date': article_data['publish_date'].isoformat() if article_data['publish_date'] else None
                    })
                    
                except Exception as e:
                    print(f"[ERROR] Error processing article {article_url}: {e}")
                    continue
                    
        except Exception as e:
            print(f"[SCRAPER] ERROR scraping {site['name']}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ê·¸ë£¹ë³„ë¡œ ê¸°ì‚¬ í†µí•©
    consolidated_articles = []
    
    for group, group_articles in articles_by_group.items():
        if not group_articles:
            continue
            
        # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
        unique_articles = []
        seen_titles = set()
        for article in group_articles:
            if article['title'] not in seen_titles:
                seen_titles.add(article['title'])
                unique_articles.append(article)
        
        # ê° ê·¸ë£¹ì—ì„œ ìµœëŒ€ 3ê°œì˜ ì£¼ìš” ê¸°ì‚¬ë§Œ ì„ íƒ
        selected_articles = unique_articles[:3]
        
        # ê·¸ë£¹ë³„ í†µí•© ê¸°ì‚¬ ìƒì„±
        group_summary = {
            'group': group,
            'articles': selected_articles,
            'article_count': len(selected_articles),
            'sites': list(set(article['site'] for article in selected_articles)),
            'timestamp': datetime.now().isoformat(),
            'scraping_method': 'traditional',
            'execution_type': 'scheduled' if os.environ.get('GITHUB_EVENT_NAME') == 'schedule' else 'manual'
        }
        
        consolidated_articles.append(group_summary)
    
    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'data/scraped/news_{timestamp}.json'
    
    os.makedirs('data/scraped', exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated_articles, f, ensure_ascii=False, indent=2)
    
    # GitHub Actions í™˜ê²½ë³€ìˆ˜ë¡œ ë°°ì¹˜ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    is_scheduled = os.environ.get('GITHUB_EVENT_NAME') == 'schedule'
    
    # latest.json íŒŒì¼ ì—…ë°ì´íŠ¸
    latest_info = {
        'lastUpdated': datetime.now().isoformat(),
        'latestFile': f'news_{timestamp}.json',
        'scrapingMethod': 'traditional',
        'executionType': 'scheduled' if is_scheduled else 'manual'
    }
    with open('data/latest.json', 'w', encoding='utf-8') as f:
        json.dump(latest_info, f, ensure_ascii=False, indent=2)
    
    total_articles = sum(len(group['articles']) for group in consolidated_articles)
    print(f"\nScraped {total_articles} articles from {len(consolidated_articles)} groups")
    return output_file

if __name__ == "__main__":
    import sys
    from monitoring import create_execution_summary, check_and_send_notification, save_monitoring_log
    
    try:
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
        output_file = scrape_news()
        
        # ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
        summary = create_execution_summary(scraped_file=output_file)
        
        # ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ì €ì¥
        save_monitoring_log(summary)
        
        # ì•Œë¦¼ ì „ì†¡
        check_and_send_notification(summary['status'], summary)
        
        # ì„±ê³µ ì¢…ë£Œ
        sys.exit(0)
        
    except Exception as e:
        print(f"Scraping failed with error: {e}")
        
        # ì˜¤ë¥˜ ìš”ì•½ ìƒì„±
        summary = create_execution_summary(error=e)
        
        # ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ì €ì¥
        save_monitoring_log(summary)
        
        # ì˜¤ë¥˜ ì•Œë¦¼ ì „ì†¡
        check_and_send_notification('failure', summary)
        
        # ì˜¤ë¥˜ ì¢…ë£Œ
        sys.exit(1)