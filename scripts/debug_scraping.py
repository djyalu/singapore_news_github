#!/usr/bin/env python3
"""
ìŠ¤í¬ë˜í•‘ ë””ë²„ê·¸ ìŠ¤í¬ë¦½íŠ¸ - ë¬¸ì œ ì§„ë‹¨ìš©
"""
import os
import sys
import json
import requests
from datetime import datetime
import pytz
from bs4 import BeautifulSoup

def test_site_access():
    """ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì ‘ê·¼ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸í•  ì‚¬ì´íŠ¸ë“¤
    test_sites = [
        ("The Straits Times", "https://www.straitstimes.com/global"),
        ("Channel NewsAsia", "https://www.channelnewsasia.com/"),
        ("Yahoo Singapore", "https://sg.news.yahoo.com"),
        ("AsiaOne", "https://www.asiaone.com/singapore"),
        ("Mothership", "https://mothership.sg")
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    results = []
    for name, url in test_sites:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            status = response.status_code
            
            if status == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                links = soup.find_all('a', href=True)
                article_links = [l for l in links if any(kw in l.get('href', '') for kw in ['article', 'news', 'story', '/'])]
                
                print(f"âœ… {name}: ì ‘ê·¼ ì„±ê³µ (ë§í¬ {len(article_links)}ê°œ ë°œê²¬)")
                results.append({'site': name, 'status': 'success', 'links': len(article_links)})
            else:
                print(f"âš ï¸ {name}: HTTP {status}")
                results.append({'site': name, 'status': f'HTTP {status}', 'links': 0})
                
        except requests.exceptions.Timeout:
            print(f"âŒ {name}: íƒ€ì„ì•„ì›ƒ")
            results.append({'site': name, 'status': 'timeout', 'links': 0})
        except Exception as e:
            print(f"âŒ {name}: ì˜¤ë¥˜ - {str(e)[:50]}")
            results.append({'site': name, 'status': 'error', 'links': 0})
    
    return results

def check_api_status():
    """API í‚¤ ìƒíƒœ í™•ì¸"""
    print("\nğŸ“Š API ìƒíƒœ í™•ì¸")
    print("=" * 50)
    
    # í™˜ê²½ë³€ìˆ˜ í™•ì¸
    cohere_key = os.getenv('COHERE_API_KEY')
    gemini_key = os.getenv('GOOGLE_GEMINI_API_KEY')
    
    if cohere_key:
        print("âœ… Cohere API í‚¤ ì„¤ì •ë¨")
    else:
        print("âš ï¸ Cohere API í‚¤ ì—†ìŒ")
    
    if gemini_key:
        print("âœ… Gemini API í‚¤ ì„¤ì •ë¨")
    else:
        print("âš ï¸ Gemini API í‚¤ ì—†ìŒ")
    
    # ì„¤ì • íŒŒì¼ í™•ì¸
    if os.path.exists('data/settings.json'):
        with open('data/settings.json', 'r', encoding='utf-8') as f:
            settings = json.load(f)
            method = settings.get('scrapingMethod', 'unknown')
            print(f"ğŸ“ ìŠ¤í¬ë˜í•‘ ë°©ë²•: {method}")
            
            if method == 'hybrid':
                hybrid_config = settings.get('scrapingMethodOptions', {}).get('hybrid', {})
                print(f"   - API ìš°ì„ ìˆœìœ„: {hybrid_config.get('apiPriority', [])}")

def check_recent_errors():
    """ìµœê·¼ ì˜¤ë¥˜ íŒ¨í„´ í™•ì¸"""
    print("\nğŸ”´ ìµœê·¼ ì‹¤í–‰ ë¬¸ì œ ë¶„ì„")
    print("=" * 50)
    
    kst = pytz.timezone('Asia/Seoul')
    now = datetime.now(kst)
    
    # ìµœê·¼ ìŠ¤í¬ë˜í•‘ íŒŒì¼ í™•ì¸
    scraped_dir = 'data/scraped'
    recent_files = []
    
    if os.path.exists(scraped_dir):
        for file in sorted(os.listdir(scraped_dir), reverse=True)[:5]:
            if file.startswith('news_'):
                file_path = os.path.join(scraped_dir, file)
                size = os.path.getsize(file_path)
                recent_files.append((file, size))
    
    if recent_files:
        print("ìµœê·¼ ìŠ¤í¬ë˜í•‘ íŒŒì¼:")
        for file, size in recent_files:
            size_kb = size / 1024
            status = "âœ…" if size_kb > 1 else "âš ï¸"
            print(f"  {status} {file}: {size_kb:.1f} KB")
    else:
        print("âŒ ìµœê·¼ ìŠ¤í¬ë˜í•‘ íŒŒì¼ ì—†ìŒ")
    
    # ë§ˆì§€ë§‰ ì„±ê³µ ì‹œì  í™•ì¸
    if os.path.exists('data/latest.json'):
        with open('data/latest.json', 'r') as f:
            latest = json.load(f)
            last_update = datetime.fromisoformat(latest['lastUpdated'])
            days_ago = (now - last_update).days
            print(f"\në§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {days_ago}ì¼ ì „ ({latest['lastUpdated'][:10]})")

def suggest_fixes():
    """í•´ê²° ë°©ì•ˆ ì œì‹œ"""
    print("\nğŸ’¡ ê¶Œì¥ í•´ê²° ë°©ë²•")
    print("=" * 50)
    
    print("""
1. ì¦‰ì‹œ ìˆ˜ë™ í…ŒìŠ¤íŠ¸:
   python3 scripts/scraper.py
   
2. ìŠ¤í¬ë˜í•‘ ë°©ë²• ë³€ê²½ (settings.json):
   - 'hybrid' â†’ 'traditional' (API ë¬¸ì œ ì‹œ)
   - 'traditional' â†’ 'hybrid' (ì‚¬ì´íŠ¸ ì°¨ë‹¨ ì‹œ)
   
3. GitHub Actions ë¡œê·¸ í™•ì¸:
   - https://github.com/djyalu/singapore_news_github/actions
   - ìµœê·¼ ì‹¤í–‰ í´ë¦­ â†’ ìƒì„¸ ë¡œê·¸ í™•ì¸
   
4. API í‚¤ ì¬ì„¤ì •:
   - GitHub Secretsì—ì„œ API í‚¤ ì—…ë°ì´íŠ¸
   - COHERE_API_KEY, GOOGLE_GEMINI_API_KEY
    """)

if __name__ == "__main__":
    print("ğŸ”§ Singapore News Scraper ë””ë²„ê·¸")
    print("=" * 50)
    
    # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    site_results = test_site_access()
    check_api_status()
    check_recent_errors()
    suggest_fixes()
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“ˆ ì§„ë‹¨ ìš”ì•½")
    print("=" * 50)
    
    working_sites = sum(1 for r in site_results if r['status'] == 'success')
    total_sites = len(site_results)
    
    print(f"ì‚¬ì´íŠ¸ ì ‘ê·¼ì„±: {working_sites}/{total_sites} ì •ìƒ")
    
    if working_sites < total_sites / 2:
        print("âš ï¸ ëŒ€ë¶€ë¶„ì˜ ì‚¬ì´íŠ¸ ì ‘ê·¼ ì‹¤íŒ¨ - ë„¤íŠ¸ì›Œí¬ ë˜ëŠ” ë´‡ ì°¨ë‹¨ ë¬¸ì œ")
    elif working_sites == total_sites:
        print("âœ… ëª¨ë“  ì‚¬ì´íŠ¸ ì ‘ê·¼ ê°€ëŠ¥ - ìŠ¤í¬ë˜í•‘ ë¡œì§ ì ê²€ í•„ìš”")