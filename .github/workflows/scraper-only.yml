name: Scrape News Only

on:
  workflow_dispatch:  # 수동 실행만
  
permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # AI 스크래핑 고려
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Create data directories
      run: |
        mkdir -p data/scraped
        mkdir -p data/history
    
    - name: Run scraper (respects settings.json method)
      env:
        GOOGLE_GEMINI_API_KEY: ${{ secrets.GOOGLE_GEMINI_API_KEY }}
        GITHUB_EVENT_NAME: ${{ github.event_name }}
        GITHUB_ACTIONS: true  # Selenium 사용 안함
      run: |
        echo "Starting scraper with method from settings.json..."
        python scripts/scraper.py
    
    - name: Save scraped data
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Pull latest changes to avoid conflicts
        git pull origin main --rebase || echo "No remote changes to pull"
        
        git add -f data/scraped/*.json data/latest.json || echo "No scraped files to add"
        git commit -m "Update scraped data [$(date '+%Y-%m-%d %H:%M:%S')]" || echo "No changes to commit"
        
        # Retry push with pull if needed
        git push || (git pull origin main --rebase && git push)